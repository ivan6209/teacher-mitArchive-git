Outline


â€¢	 Bayesian concept learning: Discussion


â€¢	 Probabilistic models for unsupervised and 
semi-supervised category learning 

Discussion points


â€¢	 Relation to â€œBayesian classificationâ€?


â€¢	 Relation to debate between rules / logic / 
symbols and similarity / connections / 
statistics? 

â€¢	 Where do the hypothesis space and prior 
probability distribution come from? 

Discussion points


â€¢	 Relation to â€œBayesian classificationâ€? 

â€“ Causal attribution versus referential inference. 

â€“ Which is more suited to natural concept 

learning?


â€¢	 Relation to debate between rules / logic / 
symbols and similarity / connections / 
statistics? 

â€¢	 Where do the hypothesis space and prior 
probability distribution come from? 

Hierarchical priors


FH,FT 

T~ Beta(FH,FT) 

Coin 1 

T 

Coin 2 

T

...

Coin 200 

T 

d

1 

d
2 

d

3 

d

4 

d

1 

d

2 

d

3 

d

4 

d

1 

d

2 

d

3 

d

4 

â€¢  Latent structure captures what is common to all coins, 
and also their individual variability 

Hierarchical priors


P(h) 

Concept 1 

h

 

Concept 2 

h

 

... 

Concept 200 

 

h

x
1 

x
2 

x
3 

x
4 

x
1 

x
2 

x
3 

x
4 

x
1 

x
2 

x
3 

x
4 

â€¢  Latent structure captures what is common to all 

concepts, and also their individual variability

â€¢  Is this all we need?


number  knowledge


social knowledge


math/magnitude? 

P(h)


Concept 1 

h

	

Concept 2	

h

 

... 

Concept 200 

 

h

x
1 

x
2 

x
3 

x
4 

x
1 

x
2 

x
3 

x
4 

x
1 

x
2 

x
3 

x
4 

â€¢	 Hypothesis space is not just an arbitrary collection of 

hypotheses, but a principled system.


â€¢	 Far more structured than our experience with specific 

number concepts. 


Outline


â€¢	 Bayesian concept learning: Discussion


â€¢	 Probabilistic models for unsupervised and 
semi-supervised category learning 

Simple model of concept learning


â€œThis is a blicket.â€ 

â€œCan you show me the 
other blickets?â€ 

Simple model of concept learning


Other blickets.


The objects of planet Gazoob


Image removed due to copyright considerations.


Simple model of concept learning


â€œThis is a blicket.â€ 

â€œCan you show me the 
other blickets?â€ 

Learning from just one positive example is possible if:

â€“  Assume concepts refer to clusters in the world. 

â€“  Observe enough unlabeled data to identify clear clusters. 

Complications


Complications


â€¢  Outliers 

â€œThis is a blicket.â€


Complications


â€¢  Overlapping clusters 

â€œThis is a blicket.â€ 

Complications


â€¢  How many clusters? 

â€œThis is a blicket.â€


Complications


â€¢  Clusters that are not simple blobs


â€œThis is a blicket.â€ 

Complications


â€¢  Concept labels inconsistent with clusters


â€œThis is a blicket.â€ 

â€œThis is a gazzer.â€ 

Simple model of concept learning


â€¢	 Can infer a concept from just one positive 
example if: 
â€“	 Assume concepts refer to clusters in the world.


â€“	 Observe lots of unlabeled data, in order to identify 
clusters. 

â€¢	 How do we identify the clusters? 
â€“	 With no labeled data (â€œunsupervised learningâ€) 

â€“	 With sparsely labeled data (â€œsemi-supervised learningâ€) 

Unsupervised clustering with 

probabilistic models


â€¢	 Assume a simple parametric probabilistic 

model for clusters, e.g., Gaussian.


C 

x
1 

x
2 

1 | c j ) u p ( x
p ( x
p ( x | c
j ) 
p ( x  | c j ) v e  ( x P 2 
)  Vij  )	
2
2 
/( 
i	
ij 
i 

2 | c j )

11V 

P 
21 

c 
2

c 
1

V21

P11

Unsupervised clustering with 

probabilistic models


â€¢	 Assume a simple parametric probabilistic 
model for clusters, e.g., Gaussian. 

â€¢	 Two ways to characterize jth cluster:

â€“ Parameters:  Pij ,Vij
â€“ Assignments: zj 
(k)  = 1 if kth point belongs to 
cluster j, else 0. 

Unsupervised clustering with 

probabilistic models


â€¢  Chicken-and-egg problem:


â€“ Given assignments we could solve for 

maximum likelihood parameters:

(  Pij 2
(  xi 
Â¦ z 
k
k
) 
) 
j 
k
Â¦ z 
k 
) 
( 
j 
k 

Â¦ z j 
k
( 
k 
Â¦ z j
k

k 
) 
( 
) 
xi 

Pij 

k
( 

) 

2 
V  ij

Unsupervised clustering with 

probabilistic models


â€¢  Chicken-and-egg problem:


â€“ Given parameters we could solve for 

assignments zj 
(k ):

k
( 
z 
j 

k 
j c  | x ) 
(  ) 
c p 
(

,1  j 

) 

max 
arg 
j c 

,0 

else 

k
k
j  | x ) 
(  ) v  p (x ) 
( 
(
c p 

| c j )  c p 
(
j ) 

Solve for â€œbase
rateâ€ parameters: 
j )  Â¦ z 
( k 
) 
c p 
( 
j 
k

Â– 
i 

1 
2 VS 
ij 

 ( x ( k ) P 
)  V ij  )  c p 
2
2 
2 
/( 
e 
i 
ij 
( 
j )

Alternating optimization 

algorithm


0. Guess initial parameter values.

1. Given parameter estimates, solve for maximum a 
posteriori assignments zj 
(k):

k
z ) 
( 
j 

,1  j 

max 
arg 
jc 

k 
jc  | x ) 
(  )
c p 
( 

 ( x ( k ) P 
)  V ij  ) 
2 
2
2 
/( 
c p j ) 
e 
i 
ij 
( 

(  ) v Â– 
k 
j  | x ) 
c p 
( 
i 

1 
2 VS  ij
else 
,0 
2. Given assignments zj 
(k), solve for maximum 
likelihood parameter estimates: 
(   x 
(  P ij  2
Â¦ z j 
Â¦ z 
k 
k
k
k
) 
( 
( 
) 
) 
) 
xi 
j
i 
k 
k 
Â¦ z j 
Â¦ z 
k 
k
) 
( 
( 
j 
k 
k

V 2 
ij 

P ij 

j )  Â¦ z k 
) 
( 
c p 
( 
j 
k

) 

3. Go to step 1. 

Alternating optimization 

algorithm


x

z: assignments to cluster
P V p(cj): cluster parameters 

[For simplicity, assume V p(cj) fixed.] 

Alternating optimization 

algorithm


Step 0: initial parameter values


Alternating optimization 

algorithm


Step 1: update assignments


Alternating optimization 

algorithm


Step 2: update parameters


Alternating optimization 

algorithm


Step 1: update assignments


Alternating optimization 

algorithm


Step 2: update parameters


Alternating optimization 

algorithm


0. Guess initial parameter values.

1. Given parameter estimates, solve for maximum a 
posteriori assignments zj 
(k):

k
z ) 
( 
j 

,1  j 

max 
arg 
jc 

k 
jc  | x ) 
(  )
c p 
( 

 ( x ( k ) P 
)  V ij  ) 
2 
2
2 
/( 
c p 
e 
i 
ij 
( 
j ) 

(  ) v Â– 
k 
j  | x ) 
c p 
( 
i 

1 
2 VS  ij 
,0 
else 
(k), solve for maximumWhy hard
2. Given assignments zj 
assignments? 
likelihood parameter estimates: 
(   x 
(  P ij  2
Â¦ z j 
Â¦ z 
k 
k
k
k
) 
( 
( 
) 
) 
) 
xi 
j
i 
k 
k 
Â¦ z j 
Â¦ z 
k 
k
) 
( 
( 
j 
k 
k

j )  Â¦ z k 
) 
( 
c p 
( 
j 
k

V 2 
ij 

P ij 

) 

3. Go to step 1. 

EM algorithm


e

k 
) 
( 
h j

0. Guess initial parameter values T = {P , V , p(cj)}. 
1. â€œExpectationâ€ step: Given parameter estimates, 

compute expected values of assignments zj 
(k)
1 
 ( x ( k ) P 
)  V ij  )  c p 
;T ) v Â– 
2
2 
k 
2 
/( 
j  | x ) 
( 
c p 
i 
ij 
( 
j )
( 
2 VS  ij
i 
2. â€œMaximizationâ€ step: Given expected 
assignments, solve for maximum likelihood 
parameter estimates: 
Â¦ h j  xi 
k 
k
) 
) 
( 
( 
k 
P ij  Â¦ h ) 
k
( 
j 
k

(   x ) 
(  P ij  2
Â¦ h ) 
k
k
i 
j
k
Â¦ h ) 
k 
( 
j
k 

2 
V  ij 

j )  Â¦ h j
k
( 
c p 
( 
k 

) 

What EM is really about


â€¢	 Define a single probabilistic model for the 
whole data set: 
p(X |T)  Â– p(x ) 
k 
( 
k 
Â– Â¦ p(x ) 
k 
( 
j 
k

| c j ;T)  c p 
j ;T) 
(


|T) 

â€œmixture 
modelâ€ 

Â– Â¦Â– 
j
k 
i 

1	
2 VS  ij 

( x ( k )
e 
i 

)  Vij  )  c p 
P 2 
2
2
/( 
ij	
j )
( 

What EM is really about


â€¢	 Define a single probabilistic model for the 
whole data set: 
p(X |T)  Â– p(x ) 
k 
( 
k 
Â– Â¦ p(x ) 
k 
( 
j 
k

| c j ;T)  c p 
j ;T) 
(


|T) 

â€œmixture 
modelâ€ 

1	
Â– Â¦Â– 
2 VS  ij 
j
k 
i 
â€¢	 How do we maximize w.r.t. T? 

( x ( k )
e 
i 

)  Vij  )  c p 
P 2 
2
2
/( 
ij	
j )
( 

What EM is really about


â€¢	 Maximization would be simpler if we 

introduced new labeling variables Z = {zj 
(k )}:

|T)  Â–Â– p (x ) 
;T) 
k 
( 
| c j ;T)  c p 
p ( Z X 
,	
(  j 
j 
k
(  Â¦ log
|T)  Â¦ Â¦ z 
k	
) 
log p ( Z X 
,	
j 
k
j 
i 
(  Â¦ ( x  Pij ) 
Â¦ Â¦ z 
k 
k
) 
( 
) 
2
i 
j
k
j 
i 
â€¢  Problem: we donâ€™t know Z = {zj 
(k )}!

;T) 
| c j ;T)  c p 
(  j 

k
( 
x p 
(  i 

2
/(  V  )  log 
2 
ij 

c p 
j )
( 

( k ) 
z j 

) 

What EM is really about


â€¢	 Maximization expected value of the 
â€œcomplete dataâ€ loglikelihood, log p(X, Z|T ): 
â€“	E-step: Compute expectation 
)  Â¦ p (Z | X,T  ) 
Q( T T  t 
t 
) 
( 
( 
log 
) 
|	
Z 

|T ) 
p ( Z X 
,

â€“	M-step: Maximize 
T (t   )1 

( t
Q( T T 
arg 
max 
|	
T 

) 
) 

