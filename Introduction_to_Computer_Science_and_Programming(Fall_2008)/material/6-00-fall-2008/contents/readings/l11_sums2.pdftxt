6.042/18.062J Mathematics for Computer Science 
Srini Devadas and Eric Lehman 

March 15, 2005 
Lecture Notes 

Sums, Approximations, and Asymptotics II 

1  Block Stacking 

How far can a stack of identical blocks overhang the end of a table without toppling over? 
Can a block be suspended entirely beyond the tableâ€™s edge? 

Table


Physics imposes some constraints on the arrangement of the blocks.  In particular, the 
stack  falls  off  the  desk  if  its  center  of  mass  lies  beyond  the  deskâ€™s  edge.  Moreover,  the 
center of mass of the top k  blocks must lie above the (k  +  1)Â­st block; otherwise, the top k 
would fall over. 
In order to ï¬nd the best conï¬guration of blocks satisfying these constraints, weâ€™ll need 
a fact about centers of mass. 
Fact 1.  If two objects have masses m1  and m2  and centersÂ­ofÂ­mass at positions z1  and z2 , then the 
center of mass of the two objects together is at position: 
z1m1  +  z2m2 
m1  +  m2 

Deï¬ne  the  offset  of  a  particular  conï¬guration  of  blocks  to  be  the  horizonal  distance 
from its center of mass to its rightmost edge. 

offset 
? 
s-

? 

center of mass 

Table 

2 

Sums, Approximations, and Asymptotics II 

The  offset  determines  precisely  how  far  that  conï¬guration  can  extend  beyond  the  desk 
since at best the center of mass lies right above the deskâ€™s edge.  So hereafter we can focus 
on maximizing  the  offset  of  a  stack  of  n  blocks  rather  than  the  overhang.  As  a  result, we 
need only be concerned with whether the stack is internally stable and not with whether 
the whole conï¬guration is too far to the right and falls off the table. 

We  can  establish  the  greatest  possible  offset  of  a  stack  of  n  blocks with  an  inductive 
argument.  This  is an  instance where  induction not only serves as a proof  technique, but 
also turns out to be a nice tool for reasoning about the problem. 
Theorem 1.  The greatest possible offset of a stack of n â‰¥ 1  blocks is: 

1 
1
1
Xn  = + + +  . . . + 
2
4
6 

1 
2n 

Proof.  We use induction on n, the number of blocks.  Let P (n) be the proposition that the 
greatest possible offset of a stack of n â‰¥ 1  blocks is 1/2  +  1/4  +  . . . +  1/(2n). 
Base  case:  For  a  single  block,  the  center  of mass  is  distance X1  = 1/2  from  its  rightmost 
edge.  So the offset is 1/2   and P (1) is true. 
Inductive step:  For n  â‰¥  2, we assume that P (n âˆ’ 1)  is true in order to prove P (n).  A stack 
of n blocks consists of the bottom block together with a stack of n âˆ’ 1  blocks on top. 
In order to acheive the greatest possible offset with n blocks, the top nâˆ’ 1  blocks should 
themselves have the greatest possible offset, which is Xnâˆ’1 ; otherwise, we could do better 
by  replacing  the  top  n  âˆ’ 1  blocks  with  a  different  conï¬guration  that  has  greater  offset. 
Furthermore, the center of mass of the top n âˆ’ 1  blocks should lie directly above the right 
edge of  the bottom block;  otherwise, we  could do better by  sliding  the  top n âˆ’ 1  blocks 
farther to the right. 

s  Xnâˆ’1 
n âˆ’ 1  blocks 

Xnâˆ’1  +  1 
2 

s 

-

-

Weâ€™ve  now  identiï¬ed  the  conï¬guration  of  n  blocks  with  the  greatest  possible  offset. 
What  remains  is  to  determine what  that  offset  is.  To  that  end, weâ€™ll  apply  Fact  1 where 
positions are measured relative  to  the rightmost edge of  the nÂ­block stack.  In particular, 
we have n âˆ’ 1  blocks with center of mass at position Xnâˆ’1 , and 1 block with center of mass 
at position Xnâˆ’1  +  1  .  So Fact  1  implies  that  the maximum possible offset of  a  stack of n
2

Sums, Approximations, and Asymptotics II 

3 

Xnâˆ’1  Â· (n âˆ’ 1)  +  (Xnâˆ’1  + 
n 

2 ) 1Â·
1 

blocks is: 

Xn  = 

1 
=  Xnâˆ’1  + 
2n 
1
1 
1
1 
= + + +  . . . + 
2
4
6 
2n 
We use the assumption P (n âˆ’ 1)  in the last step. This proves P (n). 
The theorem follows by the principle of induction. 

1.1  Harmonic Numbers 

Sums similar to the one in Theorem 1 come up all the time in computer science.  In particÂ­
ular, 

1 
1 
1
1
+ + +  . . . + 
2
3 
1
n 
is called a harmonic sum.  Its value is called the nÂ­th harmonic number and is denoted Hn . 
2 Hn . 
In these terms, the greatest possible offset of a stack of n blocks is  1 
We  can  tabulate  the maximum  achievable  overhang with n  = 1,  2,  3  and  4 blocks  by 
computing the ï¬rst few harmonic numbers: 
# of blocks  maximum overhang 

1 

2 

3 

1
2

1
2

1
2

H1 

= 

1 
2

1 
( )
1

= 

1 
2

H2 

= 

1 
2

(

1
1

1
)+

2

= 

3 
4

H3 

=


1 
2

(

1
1

1
+  +
2

)

1
3

11 =  12

1
1
1
1
1
25
1 
(

4 
+  +  +
=  24
= 
) 
> 1
H4 
4
2
3
1
2
2
The  last  line  reveals  that we  can  suspend  the  fourth  block  beyond  the  edge  of  the  table! 
Hereâ€™s the conï¬guration that does the trick: 

1 
2 

1 
4 

1 
6 

1 
8 

4 

Sums, Approximations, and Asymptotics II 

1.2  Bounding a Sum with an Integral 

We need to know more about harmonic sums to determine what can be done with a large 
number of blocks.  Unfortunately, there is no closed form for Hn .  But, on the bright side, 
we  can get good  lower and upper bounds on harmonic  sums using a general  technique 
involving integration that applies to many other sums as well. 
Hereâ€™s  how  it works.  First, we  imagine  a  bar  graph where  the  area  of  the  k Â­th  bar  is 
equal  to  the  k Â­th  term  in  the  sum.  In  particular,  each  bar  has width  1  and  height  equal 
to the value of the k Â­th term.  For example, the bar graph corresponding to the harmonic 
sum 

1
1
1 
Hn  = + + +  . . . + 
1
2
3 

1 
n 

is shown below. 

6 

1 

1/2 

1
1

1
2

1
3

1
4 

. . . 

0

1

2

3

4 

-

n âˆ’ 1  n 

Now  the  value  of  the  sum  is  equal  to  the  area  of  the  bars,  and  we  can  estimate  the  area 
of the bars using integration.  Suppose we draw a smooth curve that runs  just below the 
bars; in fact, thereâ€™s a natural choice:  the curve described by the function y = 1/(x +  1). 

6 

1 

1/2 

y =  1/(x +  1)  

1 
1 

1 
2 

1 
3 

1 
4 

0

1

2

3

4 

-

n âˆ’ 1  n


Sums, Approximations, and Asymptotics II 

5 

The area of the bars is at least the area under this curve, so we have a lower bound on the 
nÂ­th harmonic sum: 
ï¿½1
1
1
1 
Hn  = + + +   . . . + 
2
3 
n 
â‰¥
1 
0 x +  1 
=  ln(n +  1)  

1 
n 

dx

Remember  that  n  blocks  can  overhang  the  edge  of  a  table  by 
 1 
2 Hn  block  widths.  So 
if we had,  say, a million blocks,  then  this  lower bound  implies  that we could achieve an 
overhang of at least 

ln(1, 000, 000  +  1) 
2 

= 6.907 . . . 

block widths!  In  fact,  since  the  lower bound of  1 
2 ln(n +  1)  grows  arbitrarily  large,  there

is no  limit on how  far  the  stack  can overhang.  Of  course,  this  assumes no breeze,  deforÂ­
mation  of  the  blocks,  or  gravitational  variation  as  our  stack  grows  thousands  of  miles 
high. 

We  can  get  an  upper  bound  on  the  nÂ­th  harmonic  number  by  playing  essentially  the 
same game. Now we need a curve that skims just above the bar graph. The curve deï¬ned 
by y = 1/x ï¬ts the bill. 

6 

1 

1/2 

y =  1/x 

1 
1 

1 
2 

1 
3 

1 
4 

0

1

2

3

4 

-

n âˆ’ 1  n 

The area under this curve is an upper bound on the area of the bar graph and thus on the 
nÂ­th  harmonic  sum.  But  thereâ€™s  a  problem:  the  area  under  the  curve  is  inï¬nite  because 
y = 1/x has a bad asymptote at x  = 0.  This is a common problem when bounding sums 
with integrals and thereâ€™s a simple solution:  take the exact value of the ï¬rst term (1/1)  and 
then bound the remaining terms (1/2  +  1/3  +  . . . +  1/n) with an integral.  In this case, we 

6 

get the upper bound: 

Sums, Approximations, and Asymptotics II 

ï¿½2
1
1 
1
Hn  = + + +  . . . + 
1 
3 
n  1 
â‰¤ 1  + 
1  x 
= 1  +  ln  n 

dx 

1 
n 

So even though there is no closedÂ­form for the nÂ­th harmonic sum, we now know that 
the harmonic numbers grow logarithmically: 
ln(n +  1)   â‰¤ Hn  â‰¤ 1  +  ln  n 

There  is  a  reï¬nement  to  the  integration  method  weâ€™ve  just  employed,  called  EulerÂ­
Maclaurin  summation,  which  yields  a  sequence  of  terms  that  correct  errors  in  the  inital 
estimate. This technique gives an absurdly accurate formula for harmonic numbers: 

Hn  =  ln  n +  Î³ +

1
2n 

âˆ’ 

1 
12n2 

+ 

ï¿½(n)
120n4 

The second term is Euler â€™s constant, Î³ = 0.577215664  . . ..  This is probably the third most 
important mathematical constant behind e and Ï€ .  It is somewhat mysterious; for example, 
no  one  knows whether  Î³  is  rational  or  irrational,  though  if  it  equals  a  ratio  of  integers, 
then  the  denominator must  be  at  least  10242,080  .  In  the  ï¬nal  term  of  the  formula  above, 
ï¿½(n)  denotes  some number  between  0  and  1.  Youâ€™ll never need  this  kind  of precision  in 
this course or, probably, anywhere else. 

2  The Factorial Function 

One  of  the  most  common  elements  in  messy  mathematical  expressions  is  the  factorial 
function: 
Â· Â· Â· (n âˆ’ 1)  Â·
Â·
Â·
n!  = 1 2 3
Factorial comes up particularly often  in combinatorics and probability, which happen to 
be the major topics in the remainder of 6.042. Within a few weeks, you are going to have 
factorials coming out your ears. 
ï¿½ 
ï¿½ 
ï¿½ 
ï¿½ 
A good way to deal with any product is to covert it into a sum by taking a logarithm: 
n
n
ln  f (k) 
f (k)  =
k=1 
k=1 

ln 

n

Then we can apply all our summing tools and exponentiate at the end to undo the effect

of the log.  Letâ€™s use this strategy to get bounds on n!.  First, we take a logarithm to make


Sums, Approximations, and Asymptotics II 

7 

the product into a sum: 

ln  n!   =  ln(1  Â· 2  Â· 3 Â· Â· Â· n)  
=  ln   1  +  ln   2  +  ln   3  +  .  .  .  +  ln  n 

This  sum  is  rather  nasty,  but  we  can  still  get  bounds  by  the  integration  method.  A 
picture can be extremely helpful in working out the proper bounding functions and limits 
of integration. 

6 

y  =  ln(x 

+  1)  

y  =  ln   x 

ln   3 

ln  4 

ln   n 

-

n 

n

n 

0

1

2

3

ï¿½ 
In this case, we get the bounds: 
â‰¤ 
ï¿½ ï¿½ï¿½

ln  x  dx 
1 
x  ln  x  âˆ’ x 
â‰¤ 
n 
n  ln  n  âˆ’ n  +  1 
â‰¤ 
1 
ï¿½
 ï¿½

Finally, we exponentiate to get bounds on n!. 
n

e 

4 
ï¿½ 
ï¿½ ï¿½ï¿½

0
ln   n!  â‰¤  (x   +  1) ln(x   +  1) âˆ’ (x   +  1)  
n 
ln  n!  â‰¤  (n  +  1) ln(n  +  1) âˆ’ (n  +  1) +  1 
0 
ï¿½

ï¿½


ln   n!  â‰¤ 

n  +  1 
e 

ln(x  +  1) dx

n 

Â· â‰¤ 
e

n!  â‰¤ 

Â·

e 

n+1 

This  gives  some  indication  how  big  n!  is:  about  (n/e)n  .  This  estimate  is  often  good 
ï¿½
 ï¿½

enough.  If you need a  little more  accuracy,  you  can add one more  term  to get Stirlingâ€™s 
âˆš
Formula: 
n!  â‰ˆ 
nn 
e 
Stirlingâ€™s  formula  is worth  committing  to memory.  Weâ€™ll  often  use  it  to  rewrite  expresÂ­
sions involving n!.  Now, one might object that the expression on the  left actually looks a 
lot better than the expression on the right. But thatâ€™s just an artifact of notation.  If you acÂ­
tually wanted to compute n!, youâ€™d need n  âˆ’ 1  multiplications.  However, the expression 
on  the  right  is  a  closed  form;  evaluating  it  requires  only  a  handful  of  basic  operations, 
regardless  of  the  value  of  n.  Furthermore,  when  n!  appears  inside  a  larger  expression, 

2Ï€n 

8 

Sums, Approximations, and Asymptotics II 

you usually canâ€™t do much with it.  It doesnâ€™t readily cancel or combine with other terms. 
In contrast, the expression on the right looks scary, but melds nicely into larger formulas. 
So donâ€™t be put off:  Stirlingâ€™s Formula is your friend. 
(Stepping  back  a  bit,  Stirlingâ€™s  formula  is  fairly  amazing.  Who would  guess  that  the 
product of the ï¬rst n  positive integers could be so precisely described by a formula involvÂ­
ing both e  and Ï€?) 
ï¿½  ï¿½
ï¿½ ï¿½
If youâ€™re feeling a little crazy, you might pull out these evenÂ­moreÂ­precise bounds: 
âˆš
âˆš
â‰¤  n!  â‰¤
n  n 
n  n
1/(12n+1) 
e 
e 
e
ï¿½
ï¿½100 
These bounds are ridiculously close.  For example, if n  =  100, then we get: 
ï¿½  ï¿½ï¿½  ï¿½ 
âˆš
ï¿½100 
ï¿½
200Ï€  e 1/1201 
ï¿½  ï¿½ï¿½  ï¿½ 
=1.000832... 
200Ï€  e 1/1200 
=1.000833... 
The upper bound is less than 7 hundredÂ­thousandths of 1% greater than the lower bound! 

100!   â‰¥ 

100!  â‰¤ 

100 
e 

100 
e 

âˆš

2Ï€n  

2Ï€n 

1/(12n)
e 

3  Asymptotic Notation 

Our  ï¬nal  topic  is  a  special  notationâ€”  called  asymptotic  notationâ€”  designed  to  sweep 
mathematical messes  under  the  rug.  Asymptotic  notation  is  sort  of  like  the  Force  from 
Star Wars.  Itâ€™s  everywhere.  Ttâ€™s  an  essential  part  of  the  language  of  computer  science. 
And mostly  it  is  a  tool  for  good.  At  best,  asymptotic  notation  suppresses  irrelevant deÂ­
tailss, while allowing  the essential  features of a mathematical analysis  to  shine  through. 
However, as weâ€™ll see, asymptotic notation also has an alluring dark side. 

3.1  Asymptotic Notation: The Jedi Perspective 
Suppose you want  to know how  long a  computer  takes  to multiply  two n  Ã— n  matrices. 
You could tally up all the multiplications and additions and loop variable increments and 
comparisons and perhaps  factor  in hardwareÂ­speciï¬c considerations such as page  faults 
and cache misses and branch mispredicts and ï¬‚oatingÂ­point unit availability and all this 
would give you one sort of answer.  (Whew!) Such an answer would be very accurate, but 
not  very  insightful.  Given  a  very  complicated  formula  for  the  running  time,  we would 
be hardÂ­pressed to answer basic questions: How would doubling the size of the matrices 
alter the running time? What is the biggest matrix we can handle?  Furthermore, a minor 
change in the procedure or hardware would invalidate the answer. 

Sums, Approximations, and Asymptotics II 

9 

On  the other hand, each of  the n2  entries  in  the product matrix takes about n steps  to 
Â· 
compute.  So the running time is roughly n n =  n3 .  This answer is certainly less precise, 
2
but it was easy to derive and is easy to interpret.  For example, we can see that doubling 
the size of the matrices from n Ã— n to 2n Ã— 2n would increase the running time from about 
n3  to  about  (2n)3  = 8n3â€”  a  factor of  8.  And,  assuming  a  computer performs billions of 
operations  in a  reasonable  time  (as opposed  to millions or  trillions),  the  largest matrices 
we could handle would be  roughly 1000  Ã— 1000.  Furthermore,  this approximate answer 
is independent of tiny implementation and hardware details.  It remains valid even after 
you upgrade your computer. 
So  approximate  answers  are  quite  attractive.  And  asymptotic  notation  allows  one  to 
formulate vague statements like â€œroughly n3â€ in a very precise way. 

3.2  Six FunnyÂ­Lookinâ€™ Symbols 

Asymptotic notation involves six weird little symbols: 

O  Î© Î˜ 
oh  omega 
theta 

o
littleÂ­oh 

Ï‰ 
littleÂ­omega 

âˆ¼

tilde

Weâ€™ll  focus on O , which  is  the most widely used.  The others are about  equally popular, 
except for Ï‰ , which is the JarÂ­Jar of the lot. 
Suppose that f and g are functions. Then the statement 

f (x) =  O(g(x))  

means that there exist constants x0  and c > 0  such that 
f (x)| â‰¤ c Â· g(x)
|

for all x > x0 .  Now  this deï¬nition  is pretty hairy.  But what  itâ€™s  trying  to say, with all  its 
cryptic little constants, is that f grows no faster than g . A bit more precisely, it says that f is 
at most a constant times greater than g , except maybe for small values of x.  For example, 
hereâ€™s a true statement: 

5x +  100  =  O(x) 

This holds because the left side is only about 5 times larger than the right.  Of course, for 
small  values  of  x (like  x = 1)  the  left  side  is  many  times  larger  than  the  right,  but  the 
deï¬nition of O is cleverly designed to sweep aside such inconvenient details. 
Letâ€™s  work  carefully  through  a  sequence  of  examples  involving  O in  order  to  better 
understand this deï¬nition. 

Claim 2.  5x +  100  =  O(x) 

Sums, Approximations, and Asymptotics II 
10 
Proof.  We must show that there exist constants x0  and c > 0  such that |5x + 100| â‰¤ c Â· x for 
all x > x0 . Let c =  10   and x0  =  20   and note that: 
5x +  100 â‰¤ 5x +  5x =  10x 
|
| 

for all x > 20

Hereâ€™s another claim that points out a very common error involving O notation. 

Claim 3.  x =  O(x2 ) 
Proof.  We must  show  that  there exist  constants x0  and  c > 0  such  that  |x| â‰¤  c Â· x2  for all 
x > x0 . Let c = 1  and x0  = 1  and note that 
| â‰¤ 1  Â·  2 
|
x 
x

for all x > 1

Many people fail to realize that a statement of the form f (x) =  O(g(x))   only places an 
upper  bound  on  the  growth  of  the  function  f .  So,  for  example,  youâ€™ll  often  hear  people 
say things like, â€œI canâ€™t use an algorithm that runs in O(x2 ) steps because thatâ€™s too slow.â€ 
People who say things like that are dumb and you should make fun of them until they cry. 
Okay, maybe not. But they are incorrect; we just proved that a fast algorithm that requires 
only  x steps  also  runs  in  time O(x )!  One  properly  expresses  a  lower  bound  using  the Î© 
2
notation, which weâ€™ll come to presently. 
What about the reverse of the previous claim?  Is x =  O(x)? On an informal basis, this 
2
means x2  grows no faster than x, which is false. Letâ€™s prove this formally. 

Claim 4.  x =  O(x) 
2

Proof.  We argue by contradiction; suppose that there exist constants x0  and c such that: 
2|x | â‰¤ c Â· x 

for all x > x0 

Dividing both sides of the inequality by x gives: 
x â‰¤ c 

for all x > x0 

But this is false when x = 1   +  max(x0 , c). 
We can show that x =ï¿½ O(100x) by essentially the same argument; intuitively, x grows 
2
2
quadratically,  while  100x grows  only  linearly.  Generally,  changes  in multiplicative  conÂ­
stants do not affect the validity of an assertion involving O .  However, constants in expoÂ­
nentials can be critical: 

Claim 5. 

4x  =  O(2x ) 

ï¿½
ï¿½
Sums, Approximations, and Asymptotics II 

11 

Proof.  We  argue  by  contradiction;  suppose  that  there  exist  constants  x0  and  c  > 0  such 
that: 
|4x | â‰¤ c Â· 
2x 
Dividing both sides by 2x  gives: 
2x  â‰¤ c 

for all x > x0

for all x > x0 

But this is false when x = 1   +  max(x0 , log  c). 

3.3  Asymptotic Notation and Limits 
The remaining symbols, Î˜, Î©, o, Ï‰ , and âˆ¼, all have deï¬nitions similar  to O :  â€œThere exist 
constants  blah  and  blah  such  that  for  all  blah,  suchÂ­andÂ­such  holds.â€  This may  prompt 
a  sense  of  deja  vu:  these  deï¬nitions  are  all  quite  similar  to  the  deï¬nition  of  a  limit.  In 
fact,  this  similarity  has  a  happy  consequence:  all  these  symbols  have  simple,  intuitive 
interpretations in terms of limits. These are summarized in the table below. 

Notation 

f =  O(g) 
f = Î©(g) 
f = Î˜(g) 
f âˆ¼ g
f =  o(g) 
f =  Ï‰(g) 

Intuition 

f grows no faster than g 
f grows at least as fast as g 
f and g grow at about the same rate 
f and g grow at the same rate 
f grows slower than g 
f grows faster than g 

lim  f /g 
xâ†’âˆ 
ï¿½ âˆ 
= 
=ï¿½
0
= 0, âˆ 
= 1 
= 0 
=  âˆ 

Example 

4x +  7  =  O(x2 ) 
9x2  = Î©(x)  
âˆš
8x2  +  x = Î˜(x2 ) 
x âˆ¼ x
x2  + 
2
1/n =  o(1)  
n2  =  Ï‰(n) 

This summary of asymptotic notation is valid for essentially all functions encountered 
in computer science. However, in the rare case when limnâ†’âˆ f /g does not exist or is negaÂ­
tive, one must consult the formal, nitÂ­picky deï¬nitions of these symbols. These deï¬nitions 
are provided in the notes for the recitation following this lecture. 

3.4  Enter the Dark Side 

So when someone talks asymptotics, why should you reach for your light saber? 
Asymptotic notation is so widely used to analyze algorithm running times that there is 
a temptation to â€œdesign for the notationâ€ rather than for true usefulness.  Remember that 
asymptotic notation conceals some information about an algorithmâ€™s performance while 
highlighting other  aspects.  So when  a  researcher designs  an  algorithm,  he or  she might 
make  tradeoffs  so  that  the  revealed part  look good  even  though  the  concealed parts make 
the algorithm undesirable. 

ï¿½
12 

Sums, Approximations, and Asymptotics II 

Indeed, there are some notable examples of algorithms where asymptotic notation obÂ­
scures DeathstarÂ­sized problems.  For example, there is an ingenious algorithm that mulÂ­
tiplies two n Ã— n matrices in O(n2.376 ) steps instead of the obvious O(n3 ). However, the O 
symbol conceals a constant so enormous that the naive multiplication algorithm is preferÂ­
able up to at least n =  1020 . No one seems to have ï¬gured out exactly where the crossover 
point is, but weâ€™re unlikely to come anywhere near it in this century at least. Another exÂ­
ample involves an algorithm that ï¬nds an approximate solution to the Traveling Salesman 
Problem  in  â€œnearly  linear  timeâ€.  The  running  time  to ï¬nd  an  approximation within  10% 
of optimal is around O(n log400   n).  Now, in a sense, the author â€™s claim that this is â€œnearly 
linear â€ is correct; for example: 

n log400  n =  O(n  

1.01

) 

But this is just a tad misleading since 
n log400  n ï¿½ n 

1.01 

for  all  n < 10100,000  .  These  extreme  examples  are  wellÂ­known,  but  whether  misuse  of 
asymptotic notation drives an wedge between algorithmic theory and practice more genÂ­
erally  is  a  question  you might  ponder.  The moral  is:  use  asymptotic  notation  to  clarify, 
not conceal. 
Avoid the Dark Side, you must. 

