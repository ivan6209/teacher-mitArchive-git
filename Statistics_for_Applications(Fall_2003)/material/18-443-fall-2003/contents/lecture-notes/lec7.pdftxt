Lecture  7


We  showed  that 

Î½ (S âˆ’ m(Ï‡))l âˆˆn  âˆ€ (  Î½ (S âˆ’ m(Ï‡))2 )1/2 (nI (Ï‡))1/2 . 
ï¿½ 
Next,  let us compute the left hand side.  We showed that 
Î½ l âˆˆ (X1 Ï‡) = 0 which implies 
|
ï¿½ 
that 
 

ï¿½ 
Î½ S  l âˆˆn  âˆ’ m(Ï‡)  Î½ l âˆˆ = 
n

Î½ l âˆˆ (Xi Ï‡) = 0 
|

and,  therefore, 

Î½ l âˆˆ = 
n 

Î½S  l âˆˆ .
n

Î½ (S âˆ’ m(Ï‡))l âˆˆ = 
n 
Let X  = (X1 , . . . , Xn )  and  denote  by 

ï¿½ 

ï¿½ 

ï¿½ 

ï¿½(X Ï‡) = f (X1 Ï‡) . . . f (Xn Ï‡)
|
|
|
the  joint  p.d.f.  (or  likelihood)  of  the  sample  X1 , . . . , Xn  We  can  rewrite  l âˆˆ
n 
of  this  joint  p.d.f.  as 

in  terms 

l âˆˆ = 
n 

.

ï¿½
ï¿½ Ï‡ 

Î½ S l âˆˆ = 
n 

Î½ S (X )
 

ï¿½âˆˆ (X Ï‡)
|
ï¿½(X |
Ï‡)

log ï¿½(X Ï‡) = 
|

 
log f (Xi Ï‡) = 
|

n
ï¿½  
ï¿½ Ï‡ 
i=1 
Therefore,  we  can  write 
 
ï¿½
ï¿½âˆˆ(X Ï‡) 
ï¿½âˆˆ (X |Ï‡) 
| ï¿½(X )dX
=  S (X ) 
ï¿½ 
ï¿½(X Ï‡)

ï¿½(X Ï‡) 
|

|
 
ï¿½  ï¿½
ï¿½
S (X )ï¿½âˆˆ(X |Ï‡)dX  = 
S (X )ï¿½(X Ï‡)dX  = 
|
ï¿½ Ï‡ 
Of  course,  we  integrate  with  respect  to  all  coordinates,  i.e.  dX  =  dX1  . . . dXn .  We 
ï¬nally  proved  that 
mâˆˆ (Ï‡) âˆ€  (  Î½ (S âˆ’ m(Ï‡))2 )1/2 (nI (Ï‡))1/2  = (VarÎ½ (S ))1/2 (nI (Ï‡))1/2 
ï¿½ 
28 

Î½ S (X ) = mâˆˆ (Ï‡).

ï¿½ 
ï¿½ 
ï¿½ Ï‡ 

= 

ï¿½
ï¿½
ï¿½
ï¿½
LECTURE  7. 

29 

which  implies  Rao-CrÂ´amer  inequality. 

(mâˆˆ (Ï‡))2 
. 
nI (Ï‡) 

VarÎ½ (S ) â†’ 
The  inequality  will  become  equality  only  if  there  is  equality  in  the  Cauchy  inÂ­
equality  applied  to  random  variables 
S âˆ’ m(Ï‡)  and  l âˆˆ .n
But  this  can  happen  only  if  there  exists  t = t(Ï‡)  such  that 
n 

S âˆ’ m(Ï‡) = t(Ï‡)l âˆˆ = t(Ï‡) 
n 
i=1 

 
l âˆˆ (Xi Ï‡).
|

7.1  Eï¬ƒcient  estimators. 

Deï¬nition:  Consider  statistic  S  = S (X1 , . . . , Xn)  and  let 

m(Ï‡) = 

ï¿½ 

Î½ S (X1 , . . . , Xn). 

We  say  that  S  is  an  eï¬ƒcient  estimate  of m(Ï‡)  if 

(mâˆˆ (Ï‡))2 
,
nI (Ï‡) 

ï¿½ 

Î½ (S âˆ’ m(Ï‡))2  =
i.e.  equality  holds  in  Rao-CrÂ´amerâ€™s  inequality. 
In  other words,  eï¬ƒcient  estimate  S  is  the best possible  unbiased  estimate  of m(Ï‡) 
in a sense that it achieves the smallest possible value for the average squared deviation 
Î½ (S âˆ’ m(Ï‡))2  for  all  Ï‡ .

ï¿½ 
We  also  showed  that  equality  can  be  achieved  in Rao-CrÂ´
amerâ€™s  inequality  only  if 
n 

S  = t(Ï‡) 
i=1 
for some function t(Ï‡). The statistic S 
n ) must a function of the sample 
= S (X1 , Â· Â· Â· , X
only  and  it  can  not  depend  on  Ï‡ .  This  means  that  eï¬ƒcient  estimates  do  not  always 
exist  and  they  exist  only  if  we  can  represent  the  derivative  of  log-likelihood  l âˆˆ as
n 
n 

i=1 
where  S  does  not  depend  on  Ï‡ .  In  this  case,  S  is  an  eï¬ƒcient  estimate  of m(Ï‡).


 
l âˆˆ (Xi Ï‡) + m(Ï‡)
|

 
l âˆˆ (Xi |Ï‡) = 

l âˆˆ = 
n 

S âˆ’ m(Ï‡) 
,
t(Ï‡)

LECTURE  7. 

30 

Exponential-type  families  of  distributions.  Let  us  consider  the  special  case 
of  so  called  exponential-type  families  of  distributions  that  have  p.d.f.  or  p.f.  f (x Ï‡)|
that  can  be  represented  as: 

f (x|Ï‡) = a(Ï‡)b(x)e c(Î½)d(x) . 

In  this  case  we  have, 

l âˆˆ (x Ï‡)  = 
|

=

ï¿½ 
(log a(Ï‡) + log b(x) + c(Ï‡)d(x))
ï¿½ Ï‡ 

ï¿½	
log f (x|Ï‡) = 
ï¿½ Ï‡ 

aâˆˆ (Ï‡)

+ câˆˆ (Ï‡)d(x). 
a(Ï‡) 

This  implies  that 

and 

. 

 
d(Xi ) 

n 

+ câˆˆ (Ï‡) 
i=1 
 
l âˆˆ (Xi Ï‡) âˆ’ 
|

n	

i=1 
n	
 
1 
d(Xi) = 
n 
i=1	
n1 
S  = 
n	
i=1 
then  S  will  be  an  eï¬ƒcient  estimate  of m(Ï‡). 
Example.  Consider  a  family  of  Poisson  distributions  ï¿½(âˆ‚)  with  p.f. 

 
aâˆˆ (Ï‡) 
l âˆˆ (Xi |Ï‡) = n
a(Ï‡)
n
1  
ncâˆˆ (Ï‡) 
i=1 
d(Xi )  and m(Ï‡) = 

aâˆˆ (Ï‡)
âˆ’ 
a(Ï‡)câˆˆ (Ï‡)

aâˆˆ (Ï‡)
a(Ï‡)câˆˆ (Ï‡)

If  we  take 

ï¿½ 

Î½ S  = 

âˆ‚x 
eâˆ’ï¿½  for  x = 0, 1, . . . 
f (x|âˆ‚) = 
x! 
This  can  be  expressed  as  exponential-type  distribution  if  we  write 
 
 
âˆ‚x 
1 
ï¿½
ï¿½
âˆ’ï¿½  =  eâˆ’ï¿½ 
log âˆ‚ x 
. 
exp	
e
 
  x! 
x! 
ï¿½ â›ï¿½ ï¿½ ï¿½â›ï¿½ï¿½
ï¿½â›ï¿½ï¿½
 
c(ï¿½)  d(x) 
a(ï¿½)  ï¿½â›ï¿½ï¿½
b(x) 
 
 
n
n
1 
1 
Â¯
Xi  = X 
d(Xi ) = 
n 
n
i=1 
i=1 
is eï¬ƒcient estimate of its expectation m(âˆ‚) =  ï¿½S  =  ï¿½X1  = âˆ‚. We can also compute 
ï¿½ 
its  expectation  directly  using  the  formula  above: 
âˆ’(âˆ’eâˆ’ï¿½ ) 
eâˆ’ï¿½ ( 1  = âˆ‚. 
ï¿½ ) 

aâˆˆ (âˆ‚) 
ï¿½S  = âˆ’ 
a(âˆ‚)câˆˆ (âˆ‚)

As  a  result, 

S  = 

= 

ï¿½ 

	
ï¿½
LECTURE  7. 

31 

Maximum  likelihood  estimators.  Another  interesting  consequence  of  Rao-
CrÂ´amerâ€™s  theorem  is  the  following.  Suppose  that  the MLE  Ï‡Ë† is  unbiased: 

If  we  take  S  = Ï‡Ë† and m(Ï‡) = Ï‡  then  Rao-CrÂ´amerâ€™s  inequality  implies  that 

ï¿½  Ï‡Ë† = Ï‡ . 

1 
. 
nI (Ï‡) 

Var(Ï‡Ë†) â†’ 
On  the  other hand when we  showed  asymptotic  normality  of  the MLE we  proved  the 
following  convergence  in  distribution: 
 
1  ï¿½
âˆn(Ï‡Ë† âˆ’ Ï‡) â‰ˆ N  0,
ï¿½
I (Ï‡) 
In  particular,  the  variance  of  âˆn(Ï‡Ë† âˆ’  Ï‡)  converges  to  the  variance  of  the  normal 
distribution  1/I (Ï‡),  i.e. 

. 

Var(âˆn(Ï‡Ë† âˆ’ Ï‡)) = nVar(Ï‡Ë†) â‰ˆ 
which means  that Rao-CrÂ´amerâ€™s  inequality  becomes  equality  in  the  limit.  This  propÂ­
erty  is  called  the  asymptotic  eï¬ƒciency  and  we  showed  that  unbiased  MLE  is  asympÂ­
totically  eï¬ƒcient.  In  other  words,  for  large  sample  size  n  it  is  almost  best  possible. 

1 
I (Ï‡) 

