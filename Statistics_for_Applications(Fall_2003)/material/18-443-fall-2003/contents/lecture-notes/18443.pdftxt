18.443.  Statistics  for  Applications.


by 

Dmitry  Panchenko 

Department  of  Mathematics 
Massachusetts  Institute  of  Technology 

Contents 


1  Estimation  theory. 
Introduction .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
1.1 

2

3

4 

5 

6 

7 

8 

9 

10 

2.1  Some  probability  distributions.

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

3.1  Method  of moments.

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

4.1  Maximum  likelihood  estimators. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
5.1  Consistency  of MLE.
5.2  Asymptotic  normality  of MLE.  Fisher  information.
.  .  .  .  .  .  .  .  .  . 

6.1  Rao-CrÂ´
amer  inequality.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

7.1  Eï¬ƒcient  estimators. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

8.1  Gamma  distribution.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
8.2  Beta  distribution. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

9.1  Prior  and  posterior  distributions.

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

10.1  Bayes  estimators.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
10.2  Conjugate  prior  distributions.

1

1


3

3


8

8


13

14


17

17

20


24

25 

28

29


32

32

33


35

35


38

38

39


ii 

11.1  Suï¬ƒcient  statistic.

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

42

42


45

12.1  Jointly  suï¬ƒcient  statistics.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
46

12.2  Improving  estimators  using  suï¬ƒcient  statistics.  Rao-Blackwell  theorem.  47


13.1  Minimal  jointly  suï¬ƒcient  statistics.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
13.2  Î±2  distribution.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

14.1  Estimates  of  parameters  of  normal  distribution.

.  .  .  .  .  .  .  .  .  .  .  . 

15.1  Orthogonal  transformation  of  standard  normal  sample.

.  .  .  .  .  .  .  . 

16.1  Fisher  and  Student  distributions.

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

11 

12 

13 

14 

15 

16 

17 

49

49

51


53

53


56

56


60

60


63

63


67

67

69


71

73


76

76

79


81

81

82


86

86


17.1  Conï¬dence  intervals  for  parameters  of  normal  distribution.

.  .  .  .  .  . 

18  Testing  hypotheses. 
18.1  Testing  simple  hypotheses.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
18.2  Bayes  decision  rules.

19 

20 

21 

22 

19.1  Most  powerful  test  for  two  simple  hypotheses.

.  .  .  .  .  .  .  .  .  .  .  .  . 

20.1  Randomized most  powerful  test. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
20.2  Composite  hypotheses.  Uniformly most  powerful  test. .  .  .  .  .  .  .  .  . 

21.1  Monotone  likelihood  ratio.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
21.2  One  sided  hypotheses.

22.1  One  sided  hypotheses  continued. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

iii


23 

24 

25 

23.1  Pearsonâ€™s  theorem.

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

24.1  Goodness-of-ï¬t  test.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
24.2  Goodness-of-ï¬t  for  continuous  distribution. .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

25.1  Goodness-of-ï¬t  for  composite  hypotheses. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

89

89


94

94

96


99

99


26 

103

26.1  Test  of  independence. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  103


27 

28 

27.1  Test  of  homogeneity. 

107

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  107


28.1  Kolmogorov-Smirnov  test.

110

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  110


29  Simple  linear  regression. 
29.1  Method  of  least  squares.
29.2  Simple  linear  regression.

116

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  116

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  118


30 

30.1  Joint  distribution  of  the  estimates.

120

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  120


31 

124

31.1  Statistical  inference  in  simple  linear  regression. .  .  .  .  .  .  .  .  .  .  .  .  .  124


32 

32.1  Classiï¬cation  problem.

128

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  128


iv


List  of  Figures


2.1  Poisson Distribution 

. . . . . . . . . . . . . . . . . . . . . . . . . . . 

4.1  Maximum Likelihood Estimator (MLE) . . . . . . . . . . . . . . . . . 

5.1  Maximize over Ï‡  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
5.2  Diagram (t âˆ’ 1) vs. log t  . . . . . . . . . . . . . . . . . . . . . . . . . 
5.3  Lemma: L(Ï‡) âˆ€ L(Ï‡0 ) . . . . . . . . . . . . . . . . . . . . . . . . . . . 
9.1  Prior distribution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
9.2  Posterior distribution. 
. . . . . . . . . . . . . . . . . . . . . . . . . . 

14.1 Unit Vectors Transformation. 
. . . . . . . . . . . . . . . . . . . . . . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 
14.2 Unit Vectors Fact. 

15.1 Unit Vectors. 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 

16.1 Cumulative Distribution Function. 
. . . . . . . . . . . . . . . . . . . 
17.1 P.d.f. of Î±2

nâˆ’1  distribution and Ï•  conï¬dence interval. . . . . . . . . . . 
17.2 tnâˆ’1  distribution.  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
19.1 Bayes Decision Rule.  . . . . . . . . . . . . . . . . . . . . . . . . . . . 

20.1 Graph of F (c). 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
20.2 Power function.  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 

21.1 One sided hypotheses. 
. . . . . . . . . . . . . . . . . . . . . . . . . . 
21.2 Solving for T .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 

23.1 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
23.2 Pro jections of Î¸g . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
23.3 Rotation of the coordinate system. 
. . . . . . . . . . . . . . . . . . . 

24.1 Distribution of T  under H1  and H2 . . . . . . . . . . . . . . . . . . . . 

5


15


18

19

20


36

37


54

55


58


61


64

65


72


78

80


82

84


89

93

93


95


v 

24.2  Discretizing continuous distribution. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 
24.3  Total of 4 Sub-intervals.

97

97


25.1  Free parameters of a three point distribution.
.  .  .  .  .  .  .  .  .  .  .  .  .  100

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  102

25.2  Goodness-of-ï¬t for Composite Hypotheses.

28.1  C.d.f.  and empirical d.f.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  111

28.2  The case when F  = F0 . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  113

28.3  Fn  and F0  in the example.
.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  114


29.1  The least-squares line.

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  116


31.1  Conï¬dence Interval. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  125


32.1  Example.

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  131


vi


â‡’
List  of  Tables


. . . . . . . . . . . . . . . . . . . . . . . . . . . .  103

26.1 Contingency table. 
26.2 Montana outlook poll. 
. . . . . . . . . . . . . . . . . . . . . . . . . .  106


27.1 Test of homogeneity 

. . . . . . . . . . . . . . . . . . . . . . . . . . .  107


vii


Lecture  1


Estimation  theory. 

1.1 

Introduction 

Let  us  consider  a  set  X  (probability  space)  which  is  the  set  of  possible  values  that 
some  random  variables  (random  ob ject)  may  take.  Usually  X  will  be  a  subset  of 
,
ï¿½ 
for  example  {0, 1},  [0, 1],  [0, â‰¤), 
,  etc. 
ï¿½ 
I.  Parametric  Statistics. 
We  will  start  by  considering  a  family  of  distributions  on  X : 
Î½ , Ï‡  âŠ†  ï¿½},  indexed  by  parameter  Ï‡ .  Here,  ï¿½  is  a  set  of  possible  parameters 
â€¢  {and  probability 
 
Î½  describes  chances  of  observing  values  from  subset  of X,  i.e. 
 
for A âˆ¼ X , 
Î½ (A)  is  a  probability  to  observe  a  value  from A. 
 
â€¢  Typical  ways  to  describe  a  distribution: 
â€“  probability  density  function  (p.d.f.), 
â€“  probability  function  (p.f.), 
â€“  cumulative  distribution  function  (c.d.f.). 

For example, if we denote by N (Ï•, Î´ 2 ) a normal distribution with mean Ï• and variance 
Î´ 2 ,  then  Ï‡ = (Ï•, Î´ 2)  is  a  parameter  for  this  family  and ï¿½ =  Ã— [0, â‰¤). 
ï¿½ 
Next we will assume that we are given X  = (X1 , Â· Â· Â· , Xn) - independent  identically 
distributed  (i.i.d.)  random  variables  on X , drawn  according  to  some  distribution 
Î½0 
 
from  the  above  family,  for  some  Ï‡0  âŠ†  ï¿½,  and  suppose  that  Ï‡0  is  unknown.  In  this 
setting  we  will  study  the  following  questions. 

1.  Estimation  Theory. 
Based  on  the  observations  X1 , Â· Â· Â· , Xn  we  would  like  to  estimate  unknown  paÂ­
rameter  Ï‡0 ,  i.e.  ï¬nd  Ï‡Ë† = 
Ï‡Ë†(X1 , Â· Â· Â· , X
n )  such  that  Ï‡Ë† approximates  Ï‡0 .  In  this 
case  we  also  want  to  understand  how  well  Ï‡Ë† approximates  Ï‡0 . 

1 

LECTURE  1.  ESTIMATION  THEORY. 

2 

2.  Hypothesis  Testing. 
Decide which of the hypotheses about Ï‡0  are likely or unlikely.  Typical hypotheÂ­
ses: 
â€¢  Ï‡0  = Ï‡1?  for  some  particular  Ï‡n ? 
â€¢  Ï‡0  ï¿½ Ï‡1 
â€¢  Ï‡0  = 
â‡’  Ï‡1 
Example:  In  a  simple  yes/no  vote  (or  two  candidate  vote)  our  variable  (vote) 
can  take  two  values,  i.e.  we  can  take  the  space  X  =  {0, 1}.  Then  the  distribution  is 
described  by 
  (0) = 1 âˆ’ p 
  (1) = p, 
for  some  parameter  p âŠ† ï¿½ =  [0, 1]. The  true  parameter  p0  is  unknown.  If we  conduct 
a  poll  by  picking  n  people  randomly  and  if X1 , Â· Â· Â· , Xn  are  their  votes  then: 
1.Estimation  theory.  What  is  a  natural  estimate  of  p0? 
#(1âˆˆs  among X1 , Â· Â· Â· , Xn ) 
n 

âˆ© p0 

pË† = 

 

How  close  is  Ë†p  to  p0? 
2.  Hypothesis  testing.  How  likely  or  unlikely  are  the  following: 
1 
â€¢  Hypothesis  1:  p0  >  2 
1 
â€¢  Hypothesis  2:  p0  <  2 
II.  Non-parametric  Statistics 
In  the  second  part  of  the  class  the  questions  that we  will  study  will  be  somewhat 
diï¬€erent.  We will  still assume  that the observations X  = (X1 , Â· Â· Â· , Xn) have unknown 
,  but  we  wonâ€™t  assume  that 
comes  from  a  certain  parametric  family 
distribution 
 
 
Î½ , Ï‡ âŠ† ï¿½}.  Examples  of  questions  that may  be  asked  in  this  case  are  the  following: 
{
â€¢  Does 
come  from  some  parametric  family  {  
Î½ , Ï‡ âŠ† ï¿½}? 
 
â€¢  Is 
  = 
0  for  some  speciï¬c 
0? 
 
 
If  we  have  another  sample  X âˆˆ  = (X âˆˆ
m )  then, 
âˆˆ
1 , Â· Â· Â· , X
Do X  and X âˆˆ  have  the  same  distribution?

â€¢

If  we  have  paired  observations  (X1 , Y1 ), Â· Â· Â· , (Xn , Yn):

â€¢  Are X  and  Y  independent  of  each  other? 
â€¢  Classiï¬cation/regression  problem:  predict  Y  as  a  function  of X ;  i.e., 
Y  = f (X ) +  small  error  term  . 

Lecture  2


2.1  Some  probability  distributions. 

Let  us  recall  some  common  distributions  on  the  real  line  that  will  be  used  often  in 
this  class.  We  will  deal  with  two  types  of  distributions: 

1.  Discrete 

2.  Continuous 

 

Discrete  distributions. 
Suppose  that  a  set  X  consists  of  a  countable  or  ï¬nite  number  of  points, 
X  = {a1 , a2 , a3 , Â· Â· Â·}
.
on X  can be deï¬ned  via  a  function p(x)  on X  with 

Then  a probability distribution 
the  following  properties: 
1.  0 âˆ€ p(ai ) âˆ€ 1, 
2.  ï¿½âˆ— 
i=1 p(ai ) = 1. 
p(x)  is  called  the  probability  function.  If X  is  a  random  variable with  distribution 
then p(ai ) = 
  (ai ) is a probability that X  takes value ai .  Given a function ï¿½ : X  â‰ˆ 
the  expectation  of  ï¿½(X )  is  deï¬ned  by 
 
âˆ—

ï¿½(ai )p(ai ) 
i=1 
(Absolutely)  continuous  distributions. 
is  deï¬ned  via  a  probability  density  function 
Continuous  distribution 
on 
ï¿½ 
 
 
ï¿½
âˆ—
such  that  p(X )  â†’  0  and 
(p.d.f.)  p(x)  on 
p(X )dx  = 1.  If  a  random  vari-
ï¿½ 
âˆ’âˆ—
able  X  has  distribution 
then  the  chance/probability  that  X  takes  a  value  in  the 
 

, 

 

ï¿½ 

ï¿½(X ) = 

3


ï¿½
LECTURE  2. 

4 

interval  [a, b]  is  given  by 

  b 

p(x) = 

ï¿½ 

ï¿½(X ) = 

p(x)dx. 

ï¿½(x)p(x)dx. 

ï¿½
  (X  âŠ†  [a, b]) = 
a
Clearly,  in  this  case  for  any  a  âŠ† 
  (X  =  a)  =  0.  Given  a  function
we  have 
ï¿½ 
ï¿½  : X  â‰ˆ 
,  the  expectation  of  ï¿½(X )  is  deï¬ned  by 
ï¿½ 
 
âˆ— 
ï¿½
âˆ’âˆ— 
Notation.  The  fact that a random variable X  has distribution 
  will be denoted 
. 
by X  âˆ© 
 
Example  1.  Normal (Gaussian) Distribution N (Ï•, Î´ 2 ) with mean Ï• and variance 
Î´ 2  is  a  continuous  distribution  on  with  probability  density  function: 
ï¿½ 
1 
(xâˆ’ï¿½)2 
eâˆ’ 
for  x âŠ† (âˆ’â‰¤, â‰¤).
âˆ2Î½Î´
2ï¿½2 
Normal  distribution  often  describes  continuous  random  variables  that  can  be  afÂ­
fected  by  a  sum  of many  independent  factors,  for example,  personâ€™s  height  or weight, 
ï¬‚uctuations  of  stock market,  etc.  In  this  case,  the  reason  for having  normal distribuÂ­
tion  lies  in  the  Central  Limit  Theorem. 
Example  2.  Bernoulli  Distribution  B (p)  describes  a  random  variable  that  can 
=  {0, 1}.  The  distribution  is  described  by  a 
take  only  two  possible  values,  i.e. 
X
probability  function 
  (X  = 1) = p,  p(0) = 
  (X  = 0) = 1 âˆ’ p  for  some  p âŠ†  [0, 1]. 
Example  3.  Exponential  Distribution  E (Ï•)  is  a  continuous  distribution  with 
p.d.f. 
 
ï¿½
Ï•eâˆ’Ï•x  x 
0, 
â†’
x < 0. 
0 
Here,  Ï• > 0  is  the  parameter  of  the  distribution. 
This  distribution  has  the  following  nice  property.  If  a  random  variable X  âˆ© E (Ï•) 
then  probability  that X  exceeds  level  t  for  some  t > 0  is 
 
âˆ— 
ï¿½
  (X  âŠ†  [t, â‰¤)) = 
  (X  â†’ t) = 
t 
For  s >  0,  the  probability  that  X  will  exceed  level  t + s  given  that  it  exceeded  level 
t  can  be  computed  as  follows: 

Ï•eâˆ’Ï•xdx = eâˆ’Ï•t . 

p(x) = 

p(1) = 

  (X t + s X 
â†’ 
| â†’ 

t) = 

â†’ 
  (X t + s, X 
â†’
  (X t) 
â†’
=  eâˆ’Ï•(t+s) /eâˆ’Ï•t  = eâˆ’Ï•s  = 

t) 

  (X t + s)
â†’
= 
â†’ 
  (X t)
  (X s),â†’ 

LECTURE  2. 

i.e. 

5 

t) = 

  (X t + s X 
â†’ 
| â†’ 
â†’
  (X s).
In  other  words,  if  we  think  of  X  as  a  lifetime  of  some  ob ject  in  some  random  conÂ­
ditions,  then  this  property  means  that  the  chance  that  X  will  live  longer  then  t + s 
given  that  it  lived  longer  than  t  is  the  same  as  the  chance  that  X  lives  longer  than 
t  in  the  ï¬rst  place.  Or,  if  X  is  â€œaliveâ€  at  time  t  then  it  is  like  new,  so  to  speak. 
Therefore,  some  natural  examples  that  can  be  decribed  by  exponential  distribution 
are  the  life  span  of  high  quality  products,  or  soldiers  at  war. 
Example  4.  Poisson  Distribution  ï¿½(âˆ‚)  is  a  discrete  distribution  with 

p(k) = 

= {0, 1, 2, 3, . . .},
X 
âˆ‚k 
eâˆ’ï¿½  for  k = 0, 1, 2, , . . . 
  (X  = k) = 
k ! 
Poisson  distribution  could  be  used  to  describe  the  following  random  ob jects:  the 
number  of  stars  in  a  random  area  of  the  space;  number  of misprints  in  a  typed  page; 
number of wrong connections  to your phone number; distribution of bacteria on some 
surface  or  weed  in  the  ï¬eld.  All  these  examples  share  some  common  properties  that 
give rise to a Poisson distribution.  Suppose that we count a number of random ob jects 
in  a  certain  region  T  and  this  counting  process  has  the  following  properties: 
1.  Average number  of ob jects  in any  region S  âˆ¼ T  is proportional  to the  size  of S ,
i.e.  Count(S ) = âˆ‚ S . Here  S denotes  the  size  of S,  i.e.  length, area, volume, 
|
|
|
|
ï¿½ 
etc.  Parameter  âˆ‚ > 0  represents  the  intensity  of  the  process. 

2.  Counts  on  disjoint  regions  are  independent. 

3.  Chance  to  observe  more  than  one  ob ject  in  a  small  region  is  very  small,  i.e. 
  (Count(S ) â†’ 2)  becomes  small  when  the  size  S gets  small. 
| 
|
We will  show  that  these  assumptions  will  imply  that  the number  of ob jects  in  the 
region  T ,  Count(T ),  has  Poisson  distribution  ï¿½(âˆ‚ T )  with  parameter  âˆ‚ T .
|
|
|
|

0 

T/n 

X1 

X2 

. . . . . . . 

T 
âˆ’ Counts on small subintervals 

Xn 

Figure  2.1:  Poisson  Distribution 

For simplicity,  let us assume  that the region T  is an interval  [0, T ] of length T . Let 
us  split  this  interval  into  a  large  number  n  of  small  equal  subintervals  of  length  T /n 

LECTURE  2. 

6 

and  denote  by  Xi  the  number  of  random  ob jects  in  the  ith  subinterval,  i = 1, . . . , n. 
By  the  ï¬rst  property  above, 

ï¿½ 

Xi  = 

. 

p(x) =	

ï¿½ 

Xi  = 

we  can  write, 

âˆ‚T 
n 
On  the  other  hand,  by  deï¬nition  of  expectation 
 

  (Xi  = k) = 0 + 
  (Xi  = 1) + Ï€n , 
k
kï¿½0 
 
where  Ï€n  =  ï¿½
  (Xi  =  k),  and  by  the  last  property  above  we  assume  that  Ï€n 
kï¿½2 k
becomes  small  with  n,  since  the  probability  to  observe  more  that  two  ob jects  on  the 
interval  of  size  T /n  becomes  small  as  n  becomes  large.  Combining  two  equations 
  (Xi  =  1)  âˆ…  âˆ‚ T  .  Also,  since  by  the  last  property  the  probability  that 
above  gives, 
n 
any  count  Xi  is  â†’ 2  is  small,  i.e. 
 T ï¿½
 
ï¿½
  (at  least  one Xi  â†’ 2) âˆ€ no 
n 
 
 âˆ‚T ï¿½kï¿½
n ï¿½ï¿½
âœ
n 
k
(âˆ‚T )k 
eâˆ’ï¿½T
k ! 

â‰ˆ 
Example  5:  Uniform  Distribution  U [0, Ï‡ ]  has  probability  density  function 
 
ï¿½
Finally,  let  us  recall  some  properties  of  normal  distribution.  If  a  random  variable 
X  has  normal  distribution  N (Ï•, Î´ 2 )  then  the  r.v. 
X âˆ’ Ï• 
Î´ 

  (Count(T ) = X1  +

1 
x âŠ†  [0, Ï‡ ], 
Î½ ,
0,  otherwise. 

â‰ˆ 0  as  n â‰ˆ â‰¤, 

Y  = 

âˆ© N (0, 1)
has  standard  normal  distribution.  To  see  this,  we  can  write, 
  bÏ€+Ï• 
 X âˆ’ Ï• 
  (X  âŠ†  [aÎ´ + Ï•, bÎ´ + Ï•])  =  ï¿½
1 
  ï¿½
ï¿½
[a, b] = 
aÏ€+Ï•  âˆ2Î½Î´
Î´  âŠ†
  b 
ï¿½
1 
2 y
eâˆ’  dy , 
= 
a  âˆ2Î½
2

âˆ‚T ï¿½nâˆ’k 
n 

(xâˆ’ï¿½)2 
eâˆ’ 
2ï¿½2  dx 

+ Xn  = k)	

Â· Â· Â·

âˆ… 

 
1 âˆ’

	
LECTURE  2. 

7 

ï¿½ 

1 = 

âˆ’âˆ— 

where  in  the  last  integral  we  made  a  change  of  variables  y  = (x âˆ’ Ï•)/Î´.  This,  of 
course,  means  that  Y  âˆ© N (0, 1).  The  expectation  of  Y  is 
 
âˆ— 
ï¿½
1 
2
y 
eâˆ’  dy = 0 
Y  = 
y âˆ2Î½
2
âˆ’âˆ— 
since  we  integrate  odd  function.  To  compute  the  second  moment  Y 2 ,  let  us  ï¬rst 
ï¿½ 
2 
2Î± eâˆ’ y 
is  a  probability  density  function,  it  integrates  to  1,  i.e. 
note  that  since  â‰¥1
2
 
ï¿½
If  we  integrate  this  by  parts,  we  get, 
 
 
ï¿½
ï¿½
âˆ—  1 
ï¿½ï¿½ï¿½
âˆ— 
1
âˆ’âˆ—  âˆ’ 
dy =  âˆ
1 = 
âˆ’âˆ—  âˆ2Î½
2Î½
 
=  0 + ï¿½
âˆ— 
1 
2
eâˆ’ y 
y 2 âˆ2Î½
. 
dy =  ï¿½ 
2
âˆ’âˆ— 
Thus,  the  second  moment  Y 2  = 1.  The  variance  of  Y  is 
ï¿½ 
Var(Y ) =  Y 2  âˆ’ (  Y )2  = 1 âˆ’ 0 = 1. 
ï¿½ 

y
2 
y
(âˆ’y )eâˆ’  dy 
âˆ
2
2Î½ 

âˆ—  1 
âˆ2Î½

2 
yeâˆ’ y 
2

2
y 
eâˆ’ 
2

dy . 

2

y
eâˆ’ 
2

 

âˆ— 

âˆ’âˆ— 

Y 2

ï¿½
Lecture  3


3.1  Method  of  moments. 
Consider  a  family  of  distributions  {  
Î½  :  Ï‡  âŠ†  ï¿½}  and  and  consider  a  sample  X  = 
Î½0 ,  where  Ï‡0  âŠ†  ï¿½.  We 
(X1 , . . . , Xn )  of  i.i.d.  random  variables  with  distribution 
 
assume  that Ï‡0  is unknown  and we want  to construct  an estimate  Ï‡Ë† = Ï‡Ë†
n (X1 , Â· Â· Â· , X
n )
of  Ï‡0  based  on  the  sample X. 
Let us  recall  some  standard  facts  from probability  that we be  often used  throughÂ­
out  this  course. 
â€¢  Law  of  Large  Numbers  (LLN): 
If  the  distribution  of  the  i.i.d.  sample  X1 , . . . , Xn  is  such  that  X1  has  ï¬nite 
expectation,  i.e.  X1 < â‰¤,  then  the  sample  average 
| 
|
ï¿½ 
X1  + . . . + Xn 
Â¯
â‰ˆ 
Xn  =	
X1 
ï¿½ 
n 
converges  to  the  expectation  in  some  sense,  for  example,  for  any  arbitrarily 
small  Ï€ > 0, 

Â¯
  ( Xn  âˆ’  X1 > Îµ) â‰ˆ 0  as  n â‰ˆ â‰¤.
| 
|
Convergence  in  the  above  sense  is  called  convergence  in  probability. 
Note.  Whenever  we  will  use  the  LLN  below  we  will  simply  say  that  the  avÂ­
erage  converges  to  the  expectation  and  will  not  mention  in  what  sense.  More 
mathematically  inclined  students  are  welcome  to  carry  out  these  steps  more 
rigorously,  especially  when  we  use  LLN  in  combination  with  the  Central  Limit 
Theorem. 
â€¢  Central  Limit  Theorem  (CLT): 
If  the  distribution  of  the  i.i.d.  sample  X1 , . . . , Xn  is  such  that  X1  has  ï¬nite 
expectation  and  variance,  i.e.	 |  X1
| < â‰¤  and  Var(X ) < â‰¤,  then

ï¿½ 
âˆn(Xn  âˆ’  X1 ) â‰ˆ d  N (0, Î´ 2 ) 
Â¯

ï¿½ 
8 

ï¿½
LECTURE  3. 

9 

 

b 

a

2 
x 
eâˆ’ 
2ï¿½2  dx. 

1 
âˆ2Î½Î´

converges  in  distribution  to  normal  distribution  with  zero  mean  and  variance 
2 ,  which  means  that  for  any  interval  [a, b], 
Î´
 
â‰ˆ ï¿½
  ï¿½âˆn(Xn  âˆ’  X1 ) âŠ†  [a, b] ï¿½
Â¯
ï¿½ 
Motivating  example.  Consider  a  family  of  normal  distributions 
{N (Ï•, Î´ 2 )  : Ï• âŠ† 
.â†’ 0}
ï¿½ 
Consider  a  sample  X1 , . . . , Xn  âˆ©  N (Ï•0 , Î´0
2 )  with  distribution  from  this  family  and 
suppose  that  the  parameters  Ï•0 , Î´0  are  unknown.  If  we  want  to  estimate  these  paÂ­
rameters based on the sample  then the  law of  large numbers  above provides a natural 
way  to  do  this.  Namely,  LLN  tells  us  that 
â‰ˆ 

as  n â‰ˆ â‰¤ 

Â¯
Ë†Ï• = Xn 

X1  = Ï•0

, Î´ 2 

and,  similarly, 

Î´ 2Ë† = 

+ X 2 
n

2  + . . . + X 2 
X1
n  â‰ˆ  X 2  = Var(X ) +  X 2  = Î´0
2  + Ï•2 
0 .
1 
n 
These  two  facts  imply  that 
 X1  + 
2  + 
+ Xn ï¿½2 
Â· Â· Â·
Â· Â· Â·
âˆ’ ï¿½
X1
â‰ˆ  ï¿½  X 2  âˆ’ ( ï¿½ 
n 
n 
Î´ 2  as  the  estimates  of  unknown  Ï•0 , Î´ 2  since 
It,  therefore,  makes  sense  to  take  Ë†
Ï•  and  Ë†
0
by  the  LLN  for  large  sample  size  n  these  estimates  will  approach  the  unknown  paÂ­
rameters. 
We  can  generalize  this  example  as  follows. 
Suppose  that  the  parameter  set  ï¿½ âˆ¼ 
and  suppose  that  we  can  ï¬nd  a  function 
ï¿½ 
g  : X  â‰ˆ 
such  that  a  function 
ï¿½ 

X )2  = Î´0
2 . 

m(Ï‡) = 

ï¿½ 

Î½ g (X ) : ï¿½ â‰ˆ Im(ï¿½) 
Î½  denotes  the  expectation  with  respect  to  the 

ï¿½ 

has  a  continuous  inverse  mâˆ’1 .  Here 
Î½ .  Take 
distribution 

 

 g (X1  + Â· Â· Â·
+ g (Xn) ï¿½
g) = mâˆ’1ï¿½
Ë†
Ï‡ = mâˆ’1 (Â¯
n 
as the estimate of Ï‡0 .  (Here we implicitely assumed that Â¯g  is always in the set Im(m).) 
Since  the  sample  comes  from  distribution  with  parameter  Ï‡0 ,  by  LLN  we  have 
gÂ¯ â‰ˆ 

Î½0 g (X1 ) = m(Ï‡0 ). 

ï¿½ 

ï¿½
ï¿½
ï¿½
LECTURE  3. 

10 

Since  the  inverse  mâˆ’1  is  continuous,  this  implies  that  our  estimate 
Ï‡Ë† = mâˆ’1 (Â¯g) â‰ˆ mâˆ’1 (m(Ï‡0 )) = Ï‡0 
converges  to  the  unkown  parameter  Ï‡0 . 
Typical  choices  of  the  function  g  are  g (x) = x  or  x2 .  The  quantity  X k  is  called 
ï¿½ 
the  k th  moment  of X  and,  hence,  the  name  - method  of  moments. 
Example:  Family  of  exponential  distributions  E (Ï•)  with  p.d.f. 
 
ï¿½

Ï•eâˆ’Ï•x , x 
0, 
â†’
x < 0

0, 

p(x) = 

Take  g (x) = x.  Then


m(Ï•) =  Ï• g (X ) =  Ï•X  = 
ï¿½ 

1

. 
Ï• 

( 1  is  the  expectation  of  exponential  distribution,  see  Pset  1.)  Let  us  recall  that  we 
Ï• 
can  ï¬nd  inverse  by  solving  for  Ï•  the  equation 

We  have, 

Therefore,  we  take 

1 
m(Ï•) = Î» ,  i.e.  in  our  case  = Î» . 
Ï• 

Ï• = mâˆ’1 (Î» ) = 

1 
. 
Î» 

1
Â¯
Ï• = mâˆ’1 (Â¯
g) = mâˆ’1 (X ) =  Â¯X 
Ë†

as  the  estimate  of  unkown  Ï•0 . 
Take  g (x) = x2 .  Then 

m(Ï•) =  Ï•g (X 2 ) =  Ï•X 2  = 
ï¿½ 

2 
. 
Ï•2 

The  inverse  is 

and  we  take 

 
2 
Î» 

Ï• = mâˆ’1 (Î» ) = 

ï¿½
Â¯
g) = mâˆ’1 (X 2 ) = 
Ï• = mâˆ’1 (Â¯
Ë†

as  another  estimate  of  Ï•0 . 
The  question  is,  which  estimate  is  better? 

 

2
XÂ¯2 

ï¿½

ï¿½
ï¿½
LECTURE  3. 

11 

1.	 Consistency.  We  say  that  an  estimate  Ï‡Ë† is  consistent  if  Ï‡Ë† â‰ˆ Ï‡0  in  probability 
..  We have  shown above that by construction  the estimate by method 
as n â‰ˆ â‰¤
of moments  is  always  consistent. 

2.  Asymptotic  Normality.  We  say  that  Ï‡Ë† is  asymptotically  normal  if 
âˆn(Ï‡Ë† âˆ’ Ï‡0 ) â‰ˆd  N (0, Î´ 2  )Î½0 
where  Î´ 2  ï¿½  is  called  the  asymptotic  variance  of  the  estimate  Ï‡Ë†.
Î½0 
Theorem.  The  estimate  Ï‡Ë† = mâˆ’1 (Â¯g)  by  the method  of moments  is  asymptotical ly 
normal  with  asymptotic  variance 

V arÎ½0 (g ) 
(mâˆˆ (Ï‡0 ))2 . 
Proof.  Writing  Taylor  expansion  of  the  function mâˆ’1  at  point m(Ï‡0 )  we  have 

Î´ 2 
Î½0 

= 

(mâˆ’1 )âˆˆâˆˆ (c)
2! 

g âˆ’ m(Ï‡0 ))2 
(Â¯

g) = mâˆ’1 (m(Ï‡0 )) + (mâˆ’1 )âˆˆ (m(Ï‡0 ))(Â¯
mâˆ’1 (Â¯	
g âˆ’ m(Ï‡0 )) + 
where  c âŠ†  [m(Ï‡0 ), gÂ¯].  Since  mâˆ’1 (m(Ï‡0 )) = Ï‡0 ,  we  get 
(mâˆ’1 )âˆˆâˆˆ (c)
g âˆ’ m(Ï‡0 )2 
g) âˆ’ Ï‡0  = (mâˆ’1 )âˆˆ (m(Ï‡0 ))(Â¯
mâˆ’1 (Â¯	
g âˆ’ m(Ï‡0 ) + 
)(Â¯
2! 
Let  us  prove  that  the  left  hand  side  multiplied  by  âˆn  converges  in  distribution  to 
normal  distribution. 
(mâˆ’1 )âˆˆâˆˆ (c)  1
âˆn(mâˆ’1 (Â¯
g) âˆ’ Ï‡0 ) = (mâˆ’1 )âˆˆ (m(Ï‡0 )) âˆn(Â¯	
(âˆn(Â¯
g âˆ’ m(Ï‡0 )))2
g âˆ’ m(Ï‡0 )) +
2!  âˆn  ï¿½
 
 
 
 
 
 
ï¿½
â›ï¿½
ï¿½
ï¿½
â›ï¿½
(3.1) 
Let  us  recall  that 
g (X1 ) + Â· Â· Â·
+ g (Xn) 
, g (X1) = m(Ï‡0 ). 
n 
Central  limit  theorem  tells  us  that 
âˆn(Â¯g âˆ’ m(Ï‡0 ) â‰ˆ N (0, VarÎ½0 (g (X1 ))) 
where  convergence  is  in  distribution.  First  of  all,  this  means  that  the  last  term  in 
(3.1)  converges  to  0  (in  probability),  since  it  has  another  factor  of  1/âˆn.  Also,  since 
from  calculus  the  derivative  of  the  inverse 

gÂ¯ =	

(mâˆ’1 )âˆˆ (m(Ï‡0 )) = 

1
mâˆˆ (mâˆ’1 (m(Ï‡0 ))) 

= 

1 
, 
mâˆˆ (Ï‡0 ) 

ï¿½
LECTURE  3. 

12 

the  ï¬rst  term  in  (3.1)  converges  in  distribution  to 

1 
mâˆˆ (Ï‡0 )

 

(mâˆ’1 )âˆˆ (m(Ï‡0 ))âˆn(mâˆ’1 (Â¯
g) âˆ’ Ï‡0 ) â‰ˆ 

  V arÎ½0 (g (X1 )) ï¿½
ï¿½
N (0, VarÎ½0 (g (X1 ))) = N  0,
(mâˆˆ (Ï‡0 ))2 
What  this  result  tells  us  is  that  the  smaller  V arï¿½0 (g) 
is  the  better  is  the  estimate 
mï¿½ (Î½0 ) 
Ï‡Ë† in  the  sense  that  it  has  smaller  deviations  from  the  unknown  parameter  Ï‡0  asympÂ­
totically. 

Lecture  4


Let  us  go  back  to  the  example  of  exponential  distribution  E (Ï•)  from  the  last  lecture 
and  recall  that  we  obtained  two  estimates  of  unknown  parameter  Ï•0  using  the  ï¬rst 
and  second  moment  in  the method  of moments.  We  had: 

1.  Estimate  of  Ï•0  using  ï¬rst moment: 

g (X ) = X,  m(Ï•) =  Ï• g (X ) = 
ï¿½ 

1
Ï•

1 
, Ï•1  = mâˆ’1 (Â¯
g ) =  Â¯ . 
Ë†
X 

2.  Estimate  of  Ï•  using  second  moment: 

 

g (X ) = X 2 , m(Ï•) =  Ï•g (X 2 ) = 
ï¿½ 

2
Ï•2 , Ï•2  = mâˆ’1 (Â¯
g) = 
Ë†

ï¿½
How  to  decide  which  method  is  better?  The  asymptotic  normality  result  states: 
  VarÎ½0 (g (X )) ï¿½
 
âˆn(mâˆ’1 (Â¯
ï¿½
g) âˆ’ Ï‡0 ) â‰ˆ N  0,
.
(mâˆˆ (Ï‡0 ))2 
It  makes  sense  to  compare  two  estimates  by  comparing  their  asymptotic  variance. 
Let  us  compute  it  in  both  cases: 
1.  In  the  ï¬rst  case: 

2 
Â¯ . 
X 2 

= 

1 
VarÏ•0 (g (X ))  VarÏ•0 (X ) 
Ï•2 
0  = Ï•2 
0 . 
=
1 
1 
(mâˆˆ (Ï•0 ))2 
(âˆ’ Ï•
(âˆ’ Ï•
2 )2 
2 )2
0 
0
In  the  second  case we will need  to  compute  the  fourth moment  of  the  exponential 
distribution.  This  can  be  easily  done  by  integration  by  parts  but  we  will  show  a 
diï¬€erent  way  to  do  this. 
The  moment  generating  function  of  the  distribution  E (Ï•)  is: 
 
âˆ— 
ï¿½(t) =  Ï•e tX  = ï¿½
= 
ï¿½ 
k=0 

Ï• 
Ï• âˆ’ t 

e txÏ•eâˆ’Ï•xdx = 

tk 
Ï•k , 

 

âˆ— 

0 

13 

LECTURE  4. 

14 

where  in  the  last  step  we  wrote  the  usual  Taylor  series.  On  the  other  hand,  writing 
the  Taylor  series  for  etX  we  can  write, 
âˆ— 
âˆ—
ï¿½(t) =  Ï•e tX  =  Ï• 

ï¿½ 
k=0 
k=0 
Comparing  the  two  series  above  we  get  that  the  k th  moment  of  exponential  distribuÂ­
tion  is 

 (tX )k 
k ! 

 tk 
ï¿½ 
k ! 

Ï•X k . 

= 

2.  In  the  second  case: 

ï¿½ 

Ï•X k  = 

k ! 
. 
Ï•k 

4! 
2 
2 )2 
Ï•4  âˆ’ ( Ï•
Ï•0 X 4  âˆ’ (  X 2 )2 
VarÏ•0 (g (X ))  VarÏ•0 (X 2 ) 
ï¿½ 
Ï•0 
0 
0
=
4 
4 
4 
(mâˆˆ (Ï•0 ))2 
3 )2 
3 )2 
3 )2 
(âˆ’ Ï•
(âˆ’ Ï•
(âˆ’ Ï•
0 
0
0
Since  the asymptotic variance  in the ï¬rst case  is  less  than the asymptotic variance 
in  the  second  case,  the  ï¬rst  estimator  seems  to  be  better. 

5
Ï•2 
4  0 

= 

= 

=

ï¿½ 

4.1  Maximum  likelihood  estimators. 

(Textbook,  Section  6.5) 
As  always  we  consider  a  parametric  family  of  distributions  {  
Î½ , Ï‡  âŠ†  ï¿½}.  Let 
f (X Ï‡)  be  either  a  probability  function  or  a  probability  density  function  of  the  disÂ­
|
tribution 
Î½ .  Suppose  we  are  given  a  sample  X1 , . . . , Xn  with  unknown  distribution 
 
Î½ ,  i.e.  Ï‡  is  unknown.  Let  us  consider  a  likelihood  function 

ï¿½(Ï‡) = f (X1 Ï‡) Ã— . . . Ã— f (Xn Ï‡)
|
|
seen as a function of the parameter Ï‡  only.  It has a clear  interpretation.  For example, 
if  our  distributions  are  discrete  then  the  probability  function 

 

f (x Ï‡) = 
|
 
is  a  probability  to  observe  a  point  x  and  the  likelihood  function 

Î½ (X  = x)

ï¿½(Ï‡) = f (X1 Ï‡) Ã— . . . Ã— f (Xn Ï‡) = 
Î½ (X1 ) Ã— . . . Ã— 
|
|
 
is  the  probability  to  observe  the  sample  X1 , . . . , Xn . 
In the continuous case  the likelihood  function ï¿½(Ï‡) is the probability density  funcÂ­
tion  of  the  vector  (X1 , Â· Â· Â· , Xn ). 

Î½ (X1 , Â· Â· Â· , X
n )

Î½ (Xn ) = 

 

 

ï¿½
LECTURE  4. 

15 

Deï¬nition:  (Maximum  Likelihood  Estimator.)  Let  Ï‡Ë† be  the  parameter  that 
maximizes  ï¿½(Ï‡),  i.e. 

ï¿½(Ï‡Ë†) = max ï¿½(Ï‡). 
Î½ 
Then  Ï‡Ë† is  called  the maximum  likelihood  estimator  (MLE). 
To  make  our  discussion  as  simple  as  possible,  let  us  assume  that  the  likelihood 
function  behaves  like  shown  on  the  ï¬gure  4.1,  i.e.  the  maximum  is  achieved  at  the 
unique  point  Ë†Ï‡ . 

Ï•(Î¸) 

X1, ..., Xn 

Max. Pt. 

Î˜ <âˆ’ Î¸ 

Distribution Range

^Î¸ 

Best Estimator Here (at max. of fn.) 

Figure  4.1:  Maximum  Likelihood  Estimator  (MLE) 

When ï¬nding the MLE it sometimes easier to maximize the log-likelihood function 
since 

 
log f (Xi Ï‡).
|

ï¿½(Ï‡) â‰ˆ  maximize  âˆˆ log ï¿½(Ï‡) â‰ˆ  maximize 
maximizing  ï¿½  is  equivalent  to maximizing  log ï¿½. Log-likelihood  function  can be writÂ­
ten  as 
n 

i=1 
Let  us  give  several  examples  of MLE. 
Example  1.  Bernoulli  distribution  B (p). 
X 
= {0, 1}, 
  (X  = 0) = 1 âˆ’ p,  p âŠ†  [0, 1].
  (X  = 1) = p, 
Probability  function  in  this  case  is  given  by 
 
ï¿½
x = 1, 
p, 
1 âˆ’ p,  x = 0. 
Likelihood  function 
ï¿½(p) =  f (X1 p)f (X2 p) . . . f (Xn p)

| X1 +

|
|
1ï¿½ s (1 âˆ’ p)#  of  = p 
0ï¿½ s 
#  of 
+Xn )
Â·Â·Â·+Xn (1 âˆ’ p)nâˆ’(X1+Â·Â·Â·
=  p 

log ï¿½(Ï‡) = 

f (x p) = 
|

LECTURE  4. 

16 

and  the  log-likelihood  function 
+ Xn ) log p + (n âˆ’ (X1  +
log ï¿½(p) = (X1  +
+ Xn )) log(1 âˆ’ p).
Â· Â· Â·
Â· Â· Â·
To maximize  this  over  p  let  us  ï¬nd  the  critical  point  d log 
ï¿½(p)  = 0, 
dp
1 
1
p âˆ’ (n âˆ’ (X1  +
+ Xn ) 
1 âˆ’ p 

Â· Â· Â·
Solving  this  for  p  gives,


+ Xn ))

(X1  +

= 0.

Â· Â· Â·

X

1  +

p = 

Â· Â· Â·
n 

+ Xn


Â¯
= X 

 

.


. 

(Xiâˆ’ï¿½)2 
eâˆ’ 
2ï¿½2 

1 
âˆ2Î½Î´

likelihood  function 

Â¯
and  therefore  Ë†p = X  is  the MLE.

Example  2.  Normal  distribution  N (Ï•, Î´ 2 )  has  p.d.f.

1 
(Xâˆ’ï¿½)2

eâˆ’ 
f (X |(Ï•, Î´ 2)) =
 âˆ2Î½Î´
2ï¿½2 
 
n 
ï¿½(Ï•, Î´ 2 ) = â­
i=1 
and  log-likelihood  function 
 
n 
 
(X âˆ’ Ï•)2 ï¿½
1 
ï¿½
log ï¿½(Ï•, Î´ 2 ) =  
log âˆ2Î½  âˆ’ log Î´ âˆ’ 
2Î´ 2 
i=1 
 
n
1  
1
(Xi  âˆ’ Ï•)2 . 
=  n log âˆ2Î½  âˆ’ n log Î´ âˆ’ 
2Î´ 2 
i=1 
We  want  to  maximize  the  log-likelihood  with  respect  to  Ï•  and  Î´ 2 .  First,  obviously, 
for  any  Î´  we  need  to minimize  ï¿½(Xi  âˆ’ Ï•)2  over  Ï•.  The  critical  point  condition  is 
 
 
d  
(Xi  âˆ’ Ï•)2  = âˆ’2 
(Xi  âˆ’ Ï•) = 0. 
dÏ• 
Â¯
Solving  this  for  Ï•  gives  that  Ë†Ï• = X . Next,  we  need  to maximize 
 
n
1  
1
(Xi  âˆ’ X )2 
Â¯
n log âˆ2Î½  âˆ’ n log Î´ âˆ’ 
2Î´ 2 
i=1 
over  Î´.  The  critical  point  condition  reads, 
 
1  
n 
(Xi  âˆ’ X )2  = 0 
Â¯
âˆ’ Î´
Î´ 3 
and  solving  this  for  Î´  we  get 
 
n 
1 
(Xi  âˆ’ X )2 
Î´ 2 
Â¯
Ë† = 
n 
i=1 

+

is  the MLE  of  Î´ 2 . 

Lecture  5


Let  us  give  one more  example  of MLE. 
Example  3.  The  uniform  distribution  U [0, Ï‡ ]  on  the  interval  [0, Ï‡ ]  has  p.d.f. 
 
ï¿½

1 
Î½ ,  0 âˆ€ x âˆ€ Ï‡ , 
0,  otherwise 

f (x Ï‡) = 
|

The  likelihood  function 

ï¿½(Ï‡) = 

 
f (Xi Ï‡) = 
|

n 
â­
i=1 

I (X1 , . . . , Xn  âŠ†  [0, Ï‡ ])

1 
n
Ï‡
1 
nÏ‡

= 

I (max(X1 , . . . , Xn) âˆ€ Ï‡). 
Here  the  indicator  function  I (A)  equals  to  1  if A  happens  and  0  otherwise.  What we 
wrote  is  that  the  product  of  p.d.f.  f (Xi Ï‡)  will  be  equal  to  0  if  at  least  one  of  the 
|
factors  is  0  and  this  will  happen  if  at  least  one  of Xi s  will  fall  outside  of  the  interval 
[0, Ï‡ ]  which  is  the  same  as  the maximum  among  them  exceeds  Ï‡ .  In  other  words, 

ï¿½(Ï‡) = 0  if  Ï‡ < max(X1 , . . . , Xn), 

and 

1 
if  Ï‡ â†’ max(X1 , . . . , Xn). 
nÏ‡
Therefore,  looking  at  the  ï¬gure  5.1  we  see  that  Ï‡Ë† = max(X1 , . . . , Xn )  is  the MLE. 

ï¿½(Ï‡) = 

5.1  Consistency  of  MLE. 

Why  the  MLE  Ï‡Ë† converges  to  the  unkown  parameter  Ï‡0 ?  This  is  not  immediately 
obvious  and  in  this  section  we  will  give  a  sketch  of  why  this  happens. 

17 

LECTURE  5. 

18 

Ï•(Î¸) 

Î¸ 

max(X1, ..., Xn) 

Figure  5.1:  Maximize  over  Ï‡ 

 
log f (Xi Ï‡) 
|

First  of  all, MLE  Ï‡Ë† is  a maximizer  of 
n1 
n 
i=1 
which  is  just  a  log-likelihood  function normalized by  1  (of course,  this does not aï¬€ect 
n 
the  maximization).  Ln (Ï‡)  depends  on  data.  Let  us  consider  a  function  l(X Ï‡) =|
log f (X Ï‡)  and  deï¬ne 
|
L(Ï‡) = 
Î½0 l(X |Ï‡), 
where  we  recall  that  Ï‡0  is  the  true  uknown  parameter  of  the  sample  X1 , . . . , Xn .  By 
the  law  of  large  numbers,  for  any  Ï‡ , 
Î½0 l(X Ï‡) = L(Ï‡).
Ln (Ï‡) â‰ˆ 
|
Note  that  L(Ï‡)  does  not  depend  on  the  sample,  it  only  depends  on  Ï‡ .  We  will  need 
the  following 
Lemma.  We  have,  for  any  Ï‡ , 

LnÏ‡ = 

ï¿½ 

L(Ï‡) âˆ€ L(Ï‡0 ). 
Moreover,  the  inequality  is  strict  L(Ï‡) < L(Ï‡0 )  unless 

which  means  that 

 
Î½0 . 

Î½  = 

 

 

Î½0 (f (X Ï‡) = f (X Ï‡0 )) = 1.
|
|

ï¿½
LECTURE  5. 

19 

Proof.  Let  us  consider  the  diï¬€erence 


L(Ï‡) âˆ’ L(Ï‡0 ) = 

Î½0 (log f (X |Ï‡) âˆ’ log f (X |Ï‡0 )) = 

ï¿½ 

Î½0  log 

f (X Ï‡)

|
. 
Ï‡0 ) 
f (X |

tâˆ’1 

log t 

t 

0 

1

ï¿½ 

Î½0  log 

Figure  5.2:  Diagram  (t âˆ’ 1)  vs.  log t 
Since  (t âˆ’ 1)  is  an  upper  bound  on  log t  (see  ï¬gure  5.2)  we  can  write 
  ï¿½ ï¿½ f (x Ï‡) 
 
ï¿½ f (X Ï‡) 
|
|
ï¿½
ï¿½
Ï‡0 ) âˆ’ 1 
Ï‡0 ) âˆ’ 1 = 
f (x Ï‡0 )dx 
|
ï¿½ 
Î½0 
f (X |
f (x|
 
 
ï¿½
ï¿½
f (x Ï‡)dx âˆ’ 
f (x Ï‡0 )dx = 1 âˆ’ 1 = 0.
|
|
Both  integrals  are  equal  to 1 because we  are  integrating  the probability  density  funcÂ­
tions.  This  proves  that  L(Ï‡) âˆ’ L(Ï‡0 )  âˆ€  0.  The  second  statement  of  Lemma  is  also 
clear. 

f (X Ï‡)
|
Ï‡0 )  âˆ€ 
f (X |
= 

We  will  use  this  Lemma  to  sketch  the  consistency  of  the MLE. 
Theorem:  Under  some  regularity  conditions  on  the  family  of  distributions, MLE 
Ë†Ï‡  is  consistent,  i.e.  Ï‡Ë† â‰ˆ Ï‡0  as  n â‰ˆ â‰¤. 
The  statement  of  this  Theorem  is  not  very  precise  but  but  rather  than  proving  a 
rigorous  mathematical  statement  our  goal  here  to  illustrate  the  main  idea.  MatheÂ­
matically  inclined  students  are  welcome  to  come  up  with  some  precise  statement. 
Proof. 

ï¿½
LECTURE  5. 

20 

We  have  the  following  facts: 
1.  Ï‡Ë† is  the maximizer  of  Ln (Ï‡)  (by  deï¬nition). 
2.  Ï‡0  is  the maximizer  of  L(Ï‡)  (by  Lemma). 
3.  â‰¡Ï‡  we  have  Ln (Ï‡) â‰ˆ L(Ï‡)  by  LLN. 
This  situation  is  illustrated  in ï¬gure  5.3.  Therefore,  since  two  functions Ln  and L 
are getting  closer,  the points  of maximum  should  also get  closer which  exactly means 
that  Ï‡Ë† â‰ˆ Ï‡0 . 

Ln(Î¸) 

Î¸L(  ) 

Î¸ 

^Î¸ 
MLE 

Î¸0 

Figure  5.3:  Lemma:  L(Ï‡) âˆ€ L(Ï‡0 ) 

5.2	 Asymptotic  normality  of  MLE.  Fisher  inforÂ­
mation. 

We  want  to  show  the  asymptotic  normality  of MLE,  i.e.  that 
âˆn(Ï‡Ë† âˆ’ Ï‡0 ) â‰ˆd  N (0, Î´ 2 
M LE )  for  some  Î´ 2 
M LE . 
Let  us  recall  that  above  we  deï¬ned  the  function  l(X Ï‡)  =  log f (X Ï‡).  To  simplify 
|
|
the  notations  we  will  denote  by  l âˆˆ (X Ï‡), l âˆˆâˆˆ (X Ï‡),  etc.  the  derivatives  of  l(X Ï‡)  with 
|	
|
|
respect  to  Ï‡ . 

LECTURE  5. 

21 

 

Deï¬nition.  (Fisher  information.)  Fisher  Information  of  a  random  variable  X 
with  distribution 
Î½  : Ï‡ âŠ† ï¿½}  is  deï¬ned  by 
Î½0  from  the  family  {  
ï¿½ ï¿½ 
ï¿½2
Î½0 (l âˆˆ (X |Ï‡0 ))2 
I (Ï‡0 ) = 
log f (X Ï‡0 ) 
ï¿½ 
|
Î½0 
ï¿½ Ï‡ 
Next  lemma  gives  another  often  convenient  way  to  compute  Fisher  information. 
Lemma.  We  have, 

.

ï¿½ 

Î½0 l âˆˆâˆˆ (X Ï‡0 ) ï¿½ 
|

ï¿½ 

ï¿½ 2 
Î½0  ï¿½ Ï‡2  log f (X Ï‡0 ) = âˆ’I (Ï‡0 ).
|

Proof.  First  of  all,  we  have 

l âˆˆ (X |Ï‡) = (log f (X |Ï‡))âˆˆ  = 

f âˆˆ (X Ï‡)
|
f (X |
Ï‡) 

and 

(log f (X Ï‡))âˆˆâˆˆ  = 
|
Also,  since  p.d.f.  integrates  to  1, 

f âˆˆâˆˆ (X Ï‡)
|
Ï‡)  âˆ’ 
f (X |

(f âˆˆ (X |Ï‡))2 
.
f 2 (X Ï‡)|

 

ï¿½
f (x Ï‡)dx = 1,
|
if  we  take  derivatives  of  this  equation  with  respect  to  Ï‡  (and  interchange  derivative 
and  integral,  which  can  usually  be  done)  we  will  get, 
 
 
ï¿½
ï¿½
ï¿½
ï¿½ 2 
ï¿½ 
ï¿½ Ï‡2 f (x|Ï‡)dx = 
f (x Ï‡)dx = 0  and 
|
ï¿½ Ï‡ 
To  ï¬nish  the  proof  we  write  the  following  computation 
 
ï¿½
ï¿½ 2 
(log f (x Ï‡0 ))âˆˆâˆˆf (x Ï‡0 )dx
Î½0  ï¿½ Ï‡2  log f (X Ï‡0 ) = 
|
|
|
ï¿½ 
 
ï¿½ ï¿½ f âˆˆâˆˆ (x|Ï‡0 )  ï¿½ f âˆˆ (x Ï‡0 ) ï¿½2ï¿½
|
f (x|Ï‡0 )  âˆ’ 
f (x Ï‡0 )dx
|
Ï‡0 ) 
f (x
|
 
ï¿½
Î½0 (l âˆˆ (X Ï‡0 ))2  = 0 âˆ’ I (Ï‡0  = âˆ’I (Ï‡0 ).
f âˆˆâˆˆ (x Ï‡0 )dx âˆ’ 
|
|
ï¿½ 

Î½0 l âˆˆâˆˆ (X |Ï‡0 ) = 
= 

f âˆˆâˆˆ (x Ï‡)dx = 0. 
|

= 

 

We  are  now  ready  to  prove  the main  result  of  this  section. 

ï¿½
ï¿½
ï¿½
LECTURE  5. 

22 

Theorem.  (Asymptotic  normality  of MLE.) We  have,

 
1  ï¿½
âˆn(Ï‡Ë† âˆ’ Ï‡0 ) â‰ˆ N  0,
ï¿½
I (Ï‡0 ) 
ï¿½n
Proof.  Since MLE  Ï‡Ë† is maximizer  of  Ln (Ï‡) =  1 
i=1  log f (Xi |Ï‡)  we  have, 
n 

. 

L

nâˆˆ (Ï‡Ë†) = 0. 

Let  us  use  the Mean  Value  Theorem 
f (a) âˆ’ f (b)
= f âˆˆ (c)  or  f (a) = f (b) + f âˆˆ (c)(a âˆ’ b)  for  c âŠ†  [a, b] 
a âˆ’ b 
with  f (Ï‡) = Lnâˆˆ (Ï‡), a = Ï‡Ë† and  b = Ï‡0 .  Then  we  can  write, 
n ( Ë†

0 = Lnâˆˆ (Ï‡Ë†) = Lnâˆˆ (Ï‡0 ) + Lâˆˆâˆˆ Ï‡1 )(Ï‡Ë† âˆ’ Ï‡0 )
Ë†

for  some  Ï‡Ë†1  âŠ†  [Ï‡ , Ï‡0 ].  From  here  we  get  that 
âˆnLnâˆˆ (Ï‡0 ) 
and âˆn(Ï‡Ë† âˆ’ Ï‡0 ) = âˆ’
nâˆˆ (Ï‡0 )
L
Ï‡Ë† âˆ’ Ï‡0  = 
. 
âˆ’ 
n (Ï‡Ë†1 )
n (Ï‡Ë†1 ) 
Lâˆˆâˆˆ
Lâˆˆâˆˆ
Since  by  Lemma  in  the  previous  section  Ï‡0  is  the maximizer  of  L(Ï‡),  we  have 

Lâˆˆ (Ï‡0 ) = 

ï¿½ 

Î½0 l âˆˆ (X Ï‡0 ) = 0. 
|

(5.1)

(5.2)

=  âˆn

âˆnLnâˆˆ (Ï‡0 ) =  âˆn

Therefore,  the  numerator  in  (5.1) 
 1 
n
ï¿½
ï¿½
l âˆˆ (Xi Ï‡0 ) âˆ’ 0 
(5.3) 
|
n 
i=1

 1 
 
n

 
ï¿½
ï¿½
ï¿½
ï¿½
Î½0 l âˆˆ (X1 Ï‡0 )  â‰ˆ N  0, VarÎ½0 (l âˆˆ (X1 Ï‡0 )) 
l âˆˆ (Xi Ï‡0 ) âˆ’ 
|
|
|
n 
i=1 
converges  in  distribution  by  Central  Limit  Theorem. 
Next,  let  us  consider  the  denominator  in  (5.1).  First  of  all, we  have  that  for  all  Ï‡ , 
 
1 
Î½0 l âˆˆâˆˆ (X1 Ï‡)  by  LLN. 
l âˆˆâˆˆ (Xi Ï‡) â‰ˆ 
|
|
n 
Also,  since  Ï‡Ë†1  âŠ†  [Ï‡ , Ï‡0 ]  and  by  consistency  result  of  previous  section  Ï‡Ë† â‰ˆ  Ï‡0 ,  we  have 
Ë†
Ï‡Ë†1  â‰ˆ Ï‡0 . Using  this  together  with  (5.4)  we  get 
n ( Ë†
Î½0 l âˆˆâˆˆ (X1 Ï‡0 ) = âˆ’I (Ï‡0 )  by  Lemma  above. 
Lâˆˆâˆˆ Ï‡1 ) â‰ˆ 
|
ï¿½ 

Lâˆˆâˆˆ
n (Ï‡) = 

(5.4)

 

ï¿½ 

ï¿½ 



LECTURE  5. 

23 

Combining  this  with  (5.3)  we  get 
âˆnLnâˆˆ (Ï‡0 ) 
ï¿½
1 )  â‰ˆ N 
âˆ’ 
n (Ï‡Ë†
Lâˆˆâˆˆ

Finally,  the  variance, 

  VarÎ½0 (l âˆˆ (X1 Ï‡0 )) ï¿½
(I (Ï‡0 ))2|
0,

 
. 

Î½0 (l âˆˆ (X Ï‡0 ))2  âˆ’ (  Î½0 l âˆˆ (x Ï‡0 ))2  = I (Ï‡0 ) âˆ’ 0
VarÎ½0 (l âˆˆ (X1 Ï‡0 )) = 
|
|
|
ï¿½ 
where  in  the  last  equality  we  used  the  deï¬nition  of  Fisher  information  and  (5.2). 

ï¿½
Lecture  6


Let  us  compute  Fisher  information  for  some  particular  distributions. 
Example  1.  The  family  of  Bernoulli  distributions  B (p)  has  p.f. 

and  taking  the  logarithm 

f (x p) = p x (1 âˆ’ p)1âˆ’x
|

ï¿½ 
ï¿½ p 

I (p) = âˆ’ 
ï¿½ 

1 âˆ’ x 
(1 âˆ’ p)2

log f (x p) = x log p + (1 âˆ’ x) log(1 âˆ’ p).
|
The  second  derivative  with  respect  to  parameter  p  is 
ï¿½ 2 
1 âˆ’ x
x 
x
p  âˆ’ 
, 
log f (x p) = 
ï¿½ p2  log f (x p) = âˆ’ 
p2  âˆ’ 
|
|
1 âˆ’ p 
then  we  showed  that  Fisher  information  can  be  computed  as: 
ï¿½ 2 
1 âˆ’ p 
1 
1 âˆ’ 
X
ï¿½ 
ï¿½ p2  log f (X p) = 
(1 âˆ’ p)2  = 
|
.
p(1 âˆ’ p) 
(1 âˆ’ p)2 
Â¯
The MLE of p is  Ë†p = X  and the asymptotic normality result from last lecture becomes 
âˆn( Ë†p âˆ’ p0 ) â‰ˆ N (0, p0 (1 âˆ’ p0 )) 
which,  of  course,  also  follows  directly  from  the  CLT. 
Example.  The  family  of  exponential  distributions  E (Ï•)  has  p.d.f. 
 
ï¿½

Ï•eâˆ’Ï•x , x 
0 
â†’
x < 0 
0, 

f (x Ï•) = 
|

X 
p2 

+

p 
=  +
p2 

and,  therefore,


log f (x Ï•) =  log Ï• âˆ’ Ï•x â‰¥ 
|
24 

2 
1

ï¿½
ï¿½Ï•2  log f (x Ï•) = âˆ’ 
|
Ï•2

. 

ï¿½
